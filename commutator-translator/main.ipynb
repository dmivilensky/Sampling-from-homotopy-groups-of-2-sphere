{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "freegroup_dimension = 4\n",
    "\n",
    "path = Path('results', 'commutator-translation', f'{freegroup_dimension}-free-group')\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "environ[\"WAND_DISABLED\"]        = \"true\"\n",
    "environ[\"WAND_NOTEBOOK_NAME\"]   = \"commutator-translator-long\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = '<s>'\n",
    "pad_token = '<pad>'\n",
    "comma_token = ','\n",
    "# star_token = '<star>'\n",
    "left_bracket_token = '['\n",
    "right_bracket_token = ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers import (\n",
    "    normalizers, pre_tokenizers, models, processors, trainers\n",
    ")\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "tokenizer = Tokenizer(models.WordLevel(\n",
    "    vocab = {str(x): i for i, x in enumerate(chain(range(-freegroup_dimension, 0), range(1, freegroup_dimension + 1)))}\n",
    "))\n",
    "tokenizer.add_tokens([comma_token, left_bracket_token, right_bracket_token])\n",
    "tokenizer.add_special_tokens([eos_token, pad_token])\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.WhitespaceSplit(),\n",
    "])\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"$ {eos_token}\",\n",
    "    special_tokens=[\n",
    "        (eos_token, tokenizer.token_to_id(eos_token)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer.save(str(path / 'tokenizer.json'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMMUTATOR STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from freegroup import tools\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "class Expr:\n",
    "    def __init__(self): pass\n",
    "    \n",
    "@dataclass\n",
    "class Commutator(Expr):\n",
    "    left: Expr\n",
    "    right: Expr\n",
    "\n",
    "@dataclass\n",
    "class Multiplcation(Expr):\n",
    "    children: List[Expr]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Visitor:\n",
    "    def __call__(self, expr):\n",
    "        if isinstance(expr, list): \n",
    "            return self.visit_word(expr)\n",
    "        if isinstance(expr, Commutator):\n",
    "            return self.visit_commutator(expr)\n",
    "        if isinstance(expr, Multiplcation):\n",
    "            return self.visit_mult(expr)\n",
    "        raise ValueError('Unknown expr type')\n",
    "\n",
    "    def visit_word(self, word): pass\n",
    "\n",
    "    def visit_commutator(self, commutator):\n",
    "        return list(map(self, [commutator.left, commutator.right]))\n",
    "\n",
    "    def visit_mult(self, mult):\n",
    "        return list(map(self, mult.children))\n",
    "\n",
    "\n",
    "class Normalize(Visitor):\n",
    "    def visit_word(self, word):\n",
    "        return tools.normalize(word)\n",
    "\n",
    "    @staticmethod\n",
    "    def _trim_commutees(left, right):\n",
    "\n",
    "        '''\n",
    "        [xy, x] = [y, x]\n",
    "        [xy, X] = [y, X]\n",
    "        [xy, Y] = [y, x]\n",
    "        [x, xy] = [x, y]\n",
    "        [X, xy] = [X, y]\n",
    "        [Y, xy] = [x, y]\n",
    "        '''\n",
    "\n",
    "        min_length = min(len(left), len(right))\n",
    "\n",
    "        if min_length == 0:\n",
    "            return left, right\n",
    "        \n",
    "        is_right_min = len(right) == min_length\n",
    "        if not is_right_min:\n",
    "            left, right = right, left\n",
    "\n",
    "        if left[:min_length] == right:\n",
    "            left, right = Normalize._trim_commutees(left[min_length:], right)\n",
    "        if left[:min_length] == tools.reciprocal(right):\n",
    "            left, right = Normalize._trim_commutees(left[min_length:], right)\n",
    "        if left[-min_length:] == tools.reciprocal(right):\n",
    "            left, right = Normalize._trim_commutees(left[-min_length:], left[:-min_length])\n",
    "        \n",
    "        if not is_right_min:\n",
    "            return right, left\n",
    "        return left, right\n",
    "\n",
    "    def visit_commutator(self, commutator):\n",
    "        left, right = super().visit_commutator(commutator)\n",
    "\n",
    "        if isinstance(left, list) and isinstance(right, list):\n",
    "            left, right = Normalize._trim_commutees(left, right)\n",
    "\n",
    "        for child in [left, right]:\n",
    "            if isinstance(child, list) and not child:\n",
    "                return []\n",
    "\n",
    "        return Commutator(left, right)\n",
    "\n",
    "    def visit_mult(self, mult):\n",
    "        children = []\n",
    "        for child in super().visit_mult(mult):\n",
    "            if isinstance(child, Multiplcation):\n",
    "                children.extend(child.children)\n",
    "            elif children and isinstance(children[-1], list) and isinstance(child, list):\n",
    "                merged = tools.multiply(children.pop(), child)\n",
    "                children.append(self.visit_word(merged))\n",
    "            elif isinstance(child, list) and not child:\n",
    "                continue\n",
    "            else:\n",
    "                children.append(child)\n",
    "        if not children:\n",
    "            return []\n",
    "        return Multiplcation(children) if len(children) > 1 else children[0]\n",
    "\n",
    "normalize = Normalize()\n",
    "        \n",
    "\n",
    "class ToFreegroup(Visitor):\n",
    "    def visit_word(self, word):\n",
    "        return word\n",
    "\n",
    "    def visit_commutator(self, commutator):\n",
    "        left, right = super().visit_commutator(commutator)\n",
    "        return tools.commutator(left, right)\n",
    "\n",
    "    def visit_mult(self, mult):\n",
    "        return reduce(tools.multiply, super().visit_mult(mult), [])\n",
    "\n",
    "to_freegroup = ToFreegroup()\n",
    "\n",
    "\n",
    "class MaxCommuteeLength(Visitor):\n",
    "    def visit_word(self, word):\n",
    "        return len(word)\n",
    "\n",
    "    def visit_commutator(self, commutator):\n",
    "        return max(super().visit_commutator(commutator))\n",
    "\n",
    "    def visit_mult(self, mult):\n",
    "        return max(super().visit_mult(mult))\n",
    "\n",
    "class CalculateDepth(Visitor):\n",
    "    def visit_word(self, word):\n",
    "        return 0\n",
    "\n",
    "    def visit_commutator(self, commutator):\n",
    "        return max(*super().visit_commutator(commutator)) + 1\n",
    "    \n",
    "    def visit_mult(self, mult):\n",
    "        return max(super().visit_mult(mult))\n",
    "\n",
    "calculate_depth = CalculateDepth()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, fields\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ToString(Visitor):\n",
    "\n",
    "    begin_commutator_token: str     = left_bracket_token\n",
    "    end_commutator_token: str       = right_bracket_token\n",
    "    sep_commutator_token: str       = comma_token\n",
    "    begin_word_token: str           = ''\n",
    "    sep_word_token: str             = ' '\n",
    "    end_word_token: str             = ''\n",
    "    letter_fn: Callable[[int], str] = str\n",
    "    begin_multiplication_token      = ''\n",
    "    sep_multiplication_token        = ''\n",
    "    end_multiplication_token        = ''\n",
    "\n",
    "    def visit_word(self, word):\n",
    "        return f'{self.begin_word_token}{self.sep_word_token.join(map(self.letter_fn, word))}{self.end_word_token}'\n",
    "\n",
    "    def visit_commutator(self, commutator):\n",
    "        left, right = super().visit_commutator(commutator)\n",
    "        return f'{self.begin_commutator_token}{left}{self.sep_commutator_token}{right}{self.end_commutator_token}'\n",
    "\n",
    "    def visit_mult(self, mult):\n",
    "        return f'{self.begin_multiplication_token}{self.sep_multiplication_token.join(super().visit_mult(mult))}{self.end_multiplication_token}'\n",
    "\n",
    "\n",
    "import parsec\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FromString():\n",
    "    begin_commutator_token: str     = left_bracket_token\n",
    "    end_commutator_token: str       = right_bracket_token\n",
    "    sep_commutator_token: str       = comma_token\n",
    "    begin_word_token: str           = None\n",
    "    sep_word_token: str             = None\n",
    "    end_word_token: str             = None\n",
    "    letter_regex: str               = r'(-[0-9]+|[0-9]+)'\n",
    "    letter_fn: Callable[[str], int] = int\n",
    "    begin_multiplication_token: str = None\n",
    "    end_multiplication_token: str   = None\n",
    "    sep_multiplication_token: str   = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        within_spaces = lambda x: parsec.spaces() >> x << parsec.spaces()\n",
    "        token         = lambda x: within_spaces(parsec.string(x))\n",
    "        \n",
    "        @parsec.generate\n",
    "        def word():\n",
    "            \n",
    "            if not self.begin_word_token is None:\n",
    "                _ = yield token(self.begin_word_token)\n",
    "            \n",
    "            if not self.sep_word_token is None:\n",
    "                raw_word = yield parsec.sepBy1(\n",
    "                    within_spaces(parsec.regex(self.letter_regex)),\n",
    "                    token(self.sep_word_token)\n",
    "                )\n",
    "            else:\n",
    "                raw_word = yield parsec.many1(\n",
    "                    within_spaces(parsec.regex(self.letter_regex))\n",
    "                )\n",
    "            \n",
    "            if not self.end_word_token is None:\n",
    "                _ = yield within_spaces(self.end_word_token)\n",
    "\n",
    "            return list(map(self.letter_fn, raw_word))\n",
    "\n",
    "        @parsec.generate\n",
    "        def commutator():\n",
    "            \n",
    "            if not self.begin_commutator_token is None:\n",
    "                _ = yield token(self.begin_commutator_token)\n",
    "            \n",
    "            left = yield within_spaces(multiplcation)\n",
    "            \n",
    "            if not self.sep_commutator_token is None:\n",
    "                _ = yield token(self.sep_commutator_token)\n",
    "            \n",
    "            right = yield within_spaces(multiplcation)\n",
    "            \n",
    "            if not self.end_commutator_token is None:\n",
    "                _ = yield token(self.end_commutator_token)\n",
    "            \n",
    "            return Commutator(left, right)\n",
    "\n",
    "        @parsec.generate\n",
    "        def multiplcation():\n",
    "            if not self.begin_multiplication_token is None:\n",
    "                _ = yield token(self.begin_multiplication_token)\n",
    "            \n",
    "            multipliers = yield parsec.many1(within_spaces(parsec.try_choice(commutator, word)))\n",
    "            \n",
    "            if not self.end_multiplication_token is None:\n",
    "                _ = yield token(self.end_multiplication_token)\n",
    "\n",
    "            return Multiplcation(multipliers)\n",
    "        \n",
    "        self.parse = multiplcation.parse\n",
    "        self.normalize = Normalize()\n",
    "    \n",
    "    def __call__(self, string: str):\n",
    "        return self.normalize(self.parse(string))\n",
    "\n",
    "def try_or(callable, default, *args, **kwargs):\n",
    "    try:                    return callable(*args, **kwargs)\n",
    "    except BaseException:   return default\n",
    "\n",
    "\n",
    "def _to_lu_repr(number):\n",
    "    letters = 'xyzpqrst'\n",
    "    return letters[abs(number) - 1].upper() if number < 0 else letters[number - 1]\n",
    "\n",
    "def _from_lu_repr(letter):\n",
    "    letters = 'xyzpqrst'\n",
    "    return (-1 if letter.isupper() else 1) * (letters.index(letter.lower()) + 1)\n",
    "\n",
    "to_tokenizer = ToString()\n",
    "from_tokenizer = FromString()\n",
    "\n",
    "to_lu = ToString(letter_fn=_to_lu_repr, sep_commutator_token=f'{comma_token} ', sep_word_token='')\n",
    "from_lu = FromString(letter_regex=r'[a-zA-Z]{1}', letter_fn=_from_lu_repr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    freegroup_dimension: int\n",
    "    max_commutee_length: int\n",
    "    max_commutator_depth: int\n",
    "    proba_commutator: float\n",
    "    proba_multiplication: float\n",
    "    max_multipliers_number: int\n",
    "    min_total_length: int\n",
    "    max_total_length: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 427.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from freegroup import sampling as smp\n",
    "\n",
    "from itertools import repeat\n",
    "from random import randint, sample, random, shuffle\n",
    "\n",
    "config = DatasetConfig(\n",
    "    freegroup_dimension = freegroup_dimension,\n",
    "    max_commutee_length = 3,\n",
    "    max_commutator_depth = 6,\n",
    "    proba_commutator = 0.85,\n",
    "    proba_multiplication = 0.5,\n",
    "    max_multipliers_number = 3,\n",
    "    min_total_length = 0,\n",
    "    max_total_length = 120,\n",
    ")\n",
    "\n",
    "from numpy.random import geometric\n",
    "\n",
    "def commutee_fn():\n",
    "    while True: \n",
    "        yield next(smp.free_group_bounded(\n",
    "            config.freegroup_dimension,\n",
    "            geometric(0.5),\n",
    "        ))\n",
    "commutee = commutee_fn()\n",
    "\n",
    "def random_commutator(config: DatasetConfig, depth = None):\n",
    "\n",
    "    depth = smp.random_length(config.max_commutator_depth) if depth is None else depth\n",
    "\n",
    "    if depth <= 0:\n",
    "        return next(commutee)\n",
    "\n",
    "    coin = random()\n",
    "    if depth == 1 or coin < config.proba_commutator:\n",
    "        depths = [randint(1, depth), depth]\n",
    "        shuffle(depths)\n",
    "        return Commutator(*[random_commutator(config, d - 1) for d in depths])\n",
    "    coin -= config.proba_commutator\n",
    "    \n",
    "    if coin < config.proba_multiplication:\n",
    "        depths = [randint(2, depth) for _ in range(randint(2, config.max_multipliers_number) - 1)] +\\\n",
    "            [depth]\n",
    "        shuffle(depths)\n",
    "        return Multiplcation([random_commutator(config, d - 1) for d in depths])\n",
    "    coin -= config.proba_multiplication\n",
    "\n",
    "\n",
    "def commutators(config):\n",
    "    while True: yield random_commutator(config)\n",
    "        \n",
    "generator = commutators(config)\n",
    "\n",
    "generator = map(lambda x: (to_freegroup(x), x), generator)\n",
    "generator = map(lambda p: tuple(map(normalize, p)), generator)\n",
    "\n",
    "generator = filter(lambda p: not isinstance(p[1], list), generator)\n",
    "generator = filter(lambda p: config.min_total_length < len(p[0]) < config.max_total_length, generator)\n",
    "\n",
    "generator = map(lambda p: tuple(map(to_tokenizer, p)), generator)\n",
    "\n",
    "visualize = list(smp.take_unique(1000, generator, verbose=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindMultiplication(Visitor):\n",
    "    def visit_word(self, word):\n",
    "        return False\n",
    "\n",
    "    def visit_commutator(self, commutator):\n",
    "        return any(super().visit_commutator(commutator))\n",
    "\n",
    "    def visit_mult(self, mult):\n",
    "        return True\n",
    "\n",
    "class MaxCommuteeLength(Visitor):\n",
    "    def visit_word(self, word):\n",
    "        return len(word)\n",
    "\n",
    "    def visit_commutator(self, commutator):\n",
    "        return max(super().visit_commutator(commutator))\n",
    "\n",
    "    def visit_mult(self, mult):\n",
    "        return max(super().visit_mult(mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HISTORGRAM \"HOW MANY GENERATORS REDUCED\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAinklEQVR4nO3dfXBU1eH/8U9CSHjcDUGyS0oCcUQhCqhBwhbtA6REjA6U2IqTYmoZmdINFaIomUFQdAyDrSgOD2ot0KkUpTNogQGNQUMrS4QgI4KkYLGJDZtgaXYhNQ8k9/eHP+63K/iwJHHPxvdr5s6Qe87unnvMmPdsdjcxlmVZAgAAMEhspBcAAADweQQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOPERXoBl6K9vV21tbXq37+/YmJiIr0cAADwNViWpTNnziglJUWxsV/+HElUBkptba1SU1MjvQwAAHAJampqNGTIkC+dE5WB0r9/f0mfXaDD4YjwagAAwNcRDAaVmppq/xz/MlEZKOd/reNwOAgUAACizNd5eQYvkgUAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHHiIr0AEw1buD3SSwjbR8tyI70EAAA6Dc+gAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDhhB8q//vUv/exnP9PAgQPVu3dvjRo1Svv377fHLcvS4sWLNXjwYPXu3VvZ2dk6duxYyH2cPn1a+fn5cjgcSkxM1KxZs3T27NmOXw0AAOgWwgqU//znP5owYYJ69uypHTt26MiRI/rtb3+rAQMG2HOWL1+ulStXau3ataqoqFDfvn2Vk5OjpqYme05+fr4OHz6s0tJSbdu2Tbt379bs2bM776oAAEBUi7Esy/q6kxcuXKi3335bf/3rXy86blmWUlJSdN999+n++++XJAUCAblcLq1fv14zZszQBx98oIyMDO3bt09jx46VJO3cuVO33HKLPv74Y6WkpHzlOoLBoJxOpwKBgBwOx9dd/tc2bOH2Tr/PrvbRstxILwEAgC8Vzs/vsJ5B+ctf/qKxY8fqJz/5iZKTk3Xdddfp+eeft8dPnDghv9+v7Oxs+5zT6VRWVpZ8Pp8kyefzKTEx0Y4TScrOzlZsbKwqKiou+rjNzc0KBoMhBwAA6L7CCpR//OMfWrNmjYYPH67XXntNc+bM0a9//Wtt2LBBkuT3+yVJLpcr5HYul8se8/v9Sk5ODhmPi4tTUlKSPefzSkpK5HQ67SM1NTWcZQMAgCgTVqC0t7fr+uuv1+OPP67rrrtOs2fP1j333KO1a9d21fokScXFxQoEAvZRU1PTpY8HAAAiK6xAGTx4sDIyMkLOjRw5UtXV1ZIkt9stSaqrqwuZU1dXZ4+53W7V19eHjJ87d06nT5+253xeQkKCHA5HyAEAALqvsAJlwoQJqqqqCjn397//XUOHDpUkpaeny+12q6yszB4PBoOqqKiQx+ORJHk8HjU0NKiystKes2vXLrW3tysrK+uSLwQAAHQfceFMnj9/vr773e/q8ccf109/+lO98847eu655/Tcc89JkmJiYjRv3jw99thjGj58uNLT0/XQQw8pJSVF06ZNk/TZMy4333yz/auh1tZWFRYWasaMGV/rHTwAAKD7CytQbrjhBm3ZskXFxcVaunSp0tPT9dRTTyk/P9+e88ADD6ixsVGzZ89WQ0ODbrzxRu3cuVO9evWy57z44osqLCzUpEmTFBsbq7y8PK1cubLzrgoAAES1sD4HxRR8DsqF+BwUAIDpuuxzUAAAAL4JBAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjBNWoDz88MOKiYkJOUaMGGGPNzU1yev1auDAgerXr5/y8vJUV1cXch/V1dXKzc1Vnz59lJycrAULFujcuXOdczUAAKBbiAv3BldffbXeeOON/7uDuP+7i/nz52v79u3avHmznE6nCgsLNX36dL399tuSpLa2NuXm5srtdmvPnj06efKk7rrrLvXs2VOPP/54J1wOAADoDsIOlLi4OLnd7gvOBwIBvfDCC9q4caMmTpwoSVq3bp1GjhypvXv3avz48Xr99dd15MgRvfHGG3K5XLr22mv16KOP6sEHH9TDDz+s+Pj4jl8RAACIemG/BuXYsWNKSUnR5Zdfrvz8fFVXV0uSKisr1draquzsbHvuiBEjlJaWJp/PJ0ny+XwaNWqUXC6XPScnJ0fBYFCHDx/+wsdsbm5WMBgMOQAAQPcVVqBkZWVp/fr12rlzp9asWaMTJ07opptu0pkzZ+T3+xUfH6/ExMSQ27hcLvn9fkmS3+8PiZPz4+fHvkhJSYmcTqd9pKamhrNsAAAQZcL6Fc+UKVPsf48ePVpZWVkaOnSoXn75ZfXu3bvTF3decXGxioqK7K+DwSCRAgBAN9ahtxknJibqyiuv1PHjx+V2u9XS0qKGhoaQOXV1dfZrVtxu9wXv6jn/9cVe13JeQkKCHA5HyAEAALqvDgXK2bNn9eGHH2rw4MHKzMxUz549VVZWZo9XVVWpurpaHo9HkuTxeHTo0CHV19fbc0pLS+VwOJSRkdGRpQAAgG4krF/x3H///brttts0dOhQ1dbWasmSJerRo4fuvPNOOZ1OzZo1S0VFRUpKSpLD4dDcuXPl8Xg0fvx4SdLkyZOVkZGhmTNnavny5fL7/Vq0aJG8Xq8SEhK65AIBAED0CStQPv74Y915553697//rUGDBunGG2/U3r17NWjQIEnSihUrFBsbq7y8PDU3NysnJ0erV6+2b9+jRw9t27ZNc+bMkcfjUd++fVVQUKClS5d27lUBAICoFmNZlhXpRYQrGAzK6XQqEAh0yetRhi3c3un32dU+WpYb6SUAAPClwvn5zd/iAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGCcDgXKsmXLFBMTo3nz5tnnmpqa5PV6NXDgQPXr1095eXmqq6sLuV11dbVyc3PVp08fJScna8GCBTp37lxHlgIAALqRSw6Uffv26dlnn9Xo0aNDzs+fP19bt27V5s2bVV5ertraWk2fPt0eb2trU25urlpaWrRnzx5t2LBB69ev1+LFiy/9KgAAQLdySYFy9uxZ5efn6/nnn9eAAQPs84FAQC+88IKefPJJTZw4UZmZmVq3bp327NmjvXv3SpJef/11HTlyRH/84x917bXXasqUKXr00Ue1atUqtbS0dM5VAQCAqHZJgeL1epWbm6vs7OyQ85WVlWptbQ05P2LECKWlpcnn80mSfD6fRo0aJZfLZc/JyclRMBjU4cOHL/p4zc3NCgaDIQcAAOi+4sK9waZNm3TgwAHt27fvgjG/36/4+HglJiaGnHe5XPL7/fac/42T8+Pnxy6mpKREjzzySLhLBQAAUSqsZ1Bqamp077336sUXX1SvXr26ak0XKC4uViAQsI+amppv7LEBAMA3L6xAqaysVH19va6//nrFxcUpLi5O5eXlWrlypeLi4uRyudTS0qKGhoaQ29XV1cntdkuS3G73Be/qOf/1+Tmfl5CQIIfDEXIAAIDuK6xAmTRpkg4dOqSDBw/ax9ixY5Wfn2//u2fPniorK7NvU1VVperqank8HkmSx+PRoUOHVF9fb88pLS2Vw+FQRkZGJ10WAACIZmG9BqV///665pprQs717dtXAwcOtM/PmjVLRUVFSkpKksPh0Ny5c+XxeDR+/HhJ0uTJk5WRkaGZM2dq+fLl8vv9WrRokbxerxISEjrpsgAAQDQL+0WyX2XFihWKjY1VXl6empublZOTo9WrV9vjPXr00LZt2zRnzhx5PB717dtXBQUFWrp0aWcvBQAARKkYy7KsSC8iXMFgUE6nU4FAoEtejzJs4fZOv8+u9tGy3EgvAQCALxXOz2/+Fg8AADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA44QVKGvWrNHo0aPlcDjkcDjk8Xi0Y8cOe7ypqUler1cDBw5Uv379lJeXp7q6upD7qK6uVm5urvr06aPk5GQtWLBA586d65yrAQAA3UJYgTJkyBAtW7ZMlZWV2r9/vyZOnKipU6fq8OHDkqT58+dr69at2rx5s8rLy1VbW6vp06fbt29ra1Nubq5aWlq0Z88ebdiwQevXr9fixYs796oAAEBUi7Esy+rIHSQlJemJJ57Q7bffrkGDBmnjxo26/fbbJUlHjx7VyJEj5fP5NH78eO3YsUO33nqramtr5XK5JElr167Vgw8+qFOnTik+Pv5rPWYwGJTT6VQgEJDD4ejI8i9q2MLtnX6fXe2jZbmRXgIAAF8qnJ/fl/walLa2Nm3atEmNjY3yeDyqrKxUa2ursrOz7TkjRoxQWlqafD6fJMnn82nUqFF2nEhSTk6OgsGg/SwMAABAXLg3OHTokDwej5qamtSvXz9t2bJFGRkZOnjwoOLj45WYmBgy3+Vyye/3S5L8fn9InJwfPz/2RZqbm9Xc3Gx/HQwGw102AACIImE/g3LVVVfp4MGDqqio0Jw5c1RQUKAjR450xdpsJSUlcjqd9pGamtqljwcAACIr7ECJj4/XFVdcoczMTJWUlGjMmDF6+umn5Xa71dLSooaGhpD5dXV1crvdkiS3233Bu3rOf31+zsUUFxcrEAjYR01NTbjLBgAAUaTDn4PS3t6u5uZmZWZmqmfPniorK7PHqqqqVF1dLY/HI0nyeDw6dOiQ6uvr7TmlpaVyOBzKyMj4wsdISEiw39p8/gAAAN1XWK9BKS4u1pQpU5SWlqYzZ85o48aNeuutt/Taa6/J6XRq1qxZKioqUlJSkhwOh+bOnSuPx6Px48dLkiZPnqyMjAzNnDlTy5cvl9/v16JFi+T1epWQkNAlFwgAAKJPWIFSX1+vu+66SydPnpTT6dTo0aP12muv6Uc/+pEkacWKFYqNjVVeXp6am5uVk5Oj1atX27fv0aOHtm3bpjlz5sjj8ahv374qKCjQ0qVLO/eqAABAVOvw56BEAp+DciE+BwUAYLpv5HNQAAAAugqBAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA48RFegHoHMMWbo/0EsL20bLcSC8BAGAonkEBAADGIVAAAIBxCBQAAGAcAgUAABgnrEApKSnRDTfcoP79+ys5OVnTpk1TVVVVyJympiZ5vV4NHDhQ/fr1U15enurq6kLmVFdXKzc3V3369FFycrIWLFigc+fOdfxqAABAtxBWoJSXl8vr9Wrv3r0qLS1Va2urJk+erMbGRnvO/PnztXXrVm3evFnl5eWqra3V9OnT7fG2tjbl5uaqpaVFe/bs0YYNG7R+/XotXry4864KAABEtRjLsqxLvfGpU6eUnJys8vJyfe9731MgENCgQYO0ceNG3X777ZKko0ePauTIkfL5fBo/frx27NihW2+9VbW1tXK5XJKktWvX6sEHH9SpU6cUHx//lY8bDAbldDoVCATkcDgudflfKBrfshuNeJsxAHy7hPPzu0OvQQkEApKkpKQkSVJlZaVaW1uVnZ1tzxkxYoTS0tLk8/kkST6fT6NGjbLjRJJycnIUDAZ1+PDhiz5Oc3OzgsFgyAEAALqvSw6U9vZ2zZs3TxMmTNA111wjSfL7/YqPj1diYmLIXJfLJb/fb8/53zg5P35+7GJKSkrkdDrtIzU19VKXDQAAosAlB4rX69X777+vTZs2deZ6Lqq4uFiBQMA+ampquvwxAQBA5FzSR90XFhZq27Zt2r17t4YMGWKfd7vdamlpUUNDQ8izKHV1dXK73facd955J+T+zr/L5/ycz0tISFBCQsKlLBUAAEShsJ5BsSxLhYWF2rJli3bt2qX09PSQ8czMTPXs2VNlZWX2uaqqKlVXV8vj8UiSPB6PDh06pPr6entOaWmpHA6HMjIyOnItAACgmwjrGRSv16uNGzfq1VdfVf/+/e3XjDidTvXu3VtOp1OzZs1SUVGRkpKS5HA4NHfuXHk8Ho0fP16SNHnyZGVkZGjmzJlavny5/H6/Fi1aJK/Xy7MkAABAUpiBsmbNGknSD37wg5Dz69at089//nNJ0ooVKxQbG6u8vDw1NzcrJydHq1evtuf26NFD27Zt05w5c+TxeNS3b18VFBRo6dKlHbsSAADQbXToc1Aihc9B6R74HBQA+Hb5xj4HBQAAoCsQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwTtiBsnv3bt12221KSUlRTEyMXnnllZBxy7K0ePFiDR48WL1791Z2draOHTsWMuf06dPKz8+Xw+FQYmKiZs2apbNnz3boQgAAQPcRdqA0NjZqzJgxWrVq1UXHly9frpUrV2rt2rWqqKhQ3759lZOTo6amJntOfn6+Dh8+rNLSUm3btk27d+/W7NmzL/0qAABAtxIX7g2mTJmiKVOmXHTMsiw99dRTWrRokaZOnSpJ+sMf/iCXy6VXXnlFM2bM0AcffKCdO3dq3759Gjt2rCTpmWee0S233KLf/OY3SklJ6cDlAACA7qBTX4Ny4sQJ+f1+ZWdn2+ecTqeysrLk8/kkST6fT4mJiXacSFJ2drZiY2NVUVFx0fttbm5WMBgMOQAAQPfVqYHi9/slSS6XK+S8y+Wyx/x+v5KTk0PG4+LilJSUZM/5vJKSEjmdTvtITU3tzGUDAADDRMW7eIqLixUIBOyjpqYm0ksCAABdqFMDxe12S5Lq6upCztfV1dljbrdb9fX1IePnzp3T6dOn7Tmfl5CQIIfDEXIAAIDuq1MDJT09XW63W2VlZfa5YDCoiooKeTweSZLH41FDQ4MqKyvtObt27VJ7e7uysrI6czkAACBKhf0unrNnz+r48eP21ydOnNDBgweVlJSktLQ0zZs3T4899piGDx+u9PR0PfTQQ0pJSdG0adMkSSNHjtTNN9+se+65R2vXrlVra6sKCws1Y8YM3sEDAAAkXUKg7N+/Xz/84Q/tr4uKiiRJBQUFWr9+vR544AE1NjZq9uzZamho0I033qidO3eqV69e9m1efPFFFRYWatKkSYqNjVVeXp5WrlzZCZcDAAC6gxjLsqxILyJcwWBQTqdTgUCgS16PMmzh9k6/T1zoo2W5kV4CAOAbFM7P76h4Fw8AAPh2IVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGCcu0gvAt9ewhdsjvYSwfbQsN9JLAIBvBQIFCANRBQDfDH7FAwAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOHGRXgCArjVs4fZILyFsHy3LjfQSAEQYz6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAONE9F08q1at0hNPPCG/368xY8bomWee0bhx4yK5JAAGiMZ3HkUj3i0Fk0XsGZSXXnpJRUVFWrJkiQ4cOKAxY8YoJydH9fX1kVoSAAAwRMQC5cknn9Q999yju+++WxkZGVq7dq369Omj3//+95FaEgAAMEREfsXT0tKiyspKFRcX2+diY2OVnZ0tn893wfzm5mY1NzfbXwcCAUlSMBjskvW1N/+3S+4XAEzSVf8PBb7I+e85y7K+cm5EAuWTTz5RW1ubXC5XyHmXy6WjR49eML+kpESPPPLIBedTU1O7bI0A0N05n4r0CvBtdebMGTmdzi+dExUfdV9cXKyioiL76/b2dp0+fVoDBw5UTExMpz5WMBhUamqqampq5HA4OvW+v83Y167D3nYd9rZrsK9dx/S9tSxLZ86cUUpKylfOjUigXHbZZerRo4fq6upCztfV1cntdl8wPyEhQQkJCSHnEhMTu3KJcjgcRv7HjXbsa9dhb7sOe9s12NeuY/LeftUzJ+dF5EWy8fHxyszMVFlZmX2uvb1dZWVl8ng8kVgSAAAwSMR+xVNUVKSCggKNHTtW48aN01NPPaXGxkbdfffdkVoSAAAwRMQC5Y477tCpU6e0ePFi+f1+XXvttdq5c+cFL5z9piUkJGjJkiUX/EoJHcO+dh32tuuwt12Dfe063WlvY6yv814fAACAbxB/iwcAABiHQAEAAMYhUAAAgHEIFAAAYBwC5X+sWrVKw4YNU69evZSVlaV33nkn0kuKOrt379Ztt92mlJQUxcTE6JVXXgkZtyxLixcv1uDBg9W7d29lZ2fr2LFjkVlsFCkpKdENN9yg/v37Kzk5WdOmTVNVVVXInKamJnm9Xg0cOFD9+vVTXl7eBR+GiAutWbNGo0ePtj/YyuPxaMeOHfY4+9o5li1bppiYGM2bN88+x95euocfflgxMTEhx4gRI+zx7rC3BMr/99JLL6moqEhLlizRgQMHNGbMGOXk5Ki+vj7SS4sqjY2NGjNmjFatWnXR8eXLl2vlypVau3atKioq1LdvX+Xk5KipqekbXml0KS8vl9fr1d69e1VaWqrW1lZNnjxZjY2N9pz58+dr69at2rx5s8rLy1VbW6vp06dHcNXRYciQIVq2bJkqKyu1f/9+TZw4UVOnTtXhw4clsa+dYd++fXr22Wc1evTokPPsbcdcffXVOnnypH387W9/s8e6xd5asCzLssaNG2d5vV7767a2NislJcUqKSmJ4KqimyRry5Yt9tft7e2W2+22nnjiCftcQ0ODlZCQYP3pT3+KwAqjV319vSXJKi8vtyzrs33s2bOntXnzZnvOBx98YEmyfD5fpJYZtQYMGGD97ne/Y187wZkzZ6zhw4dbpaWl1ve//33r3nvvtSyL79mOWrJkiTVmzJiLjnWXveUZFEktLS2qrKxUdna2fS42NlbZ2dny+XwRXFn3cuLECfn9/pB9djqdysrKYp/DFAgEJElJSUmSpMrKSrW2tobs7YgRI5SWlsbehqGtrU2bNm1SY2OjPB4P+9oJvF6vcnNzQ/ZQ4nu2Mxw7dkwpKSm6/PLLlZ+fr+rqakndZ2+j4q8Zd7VPPvlEbW1tF3yKrcvl0tGjRyO0qu7H7/dL0kX3+fwYvlp7e7vmzZunCRMm6JprrpH02d7Gx8df8Ec02duv59ChQ/J4PGpqalK/fv20ZcsWZWRk6ODBg+xrB2zatEkHDhzQvn37Lhjje7ZjsrKytH79el111VU6efKkHnnkEd100016//33u83eEihAlPF6vXr//fdDft+Mjrnqqqt08OBBBQIB/fnPf1ZBQYHKy8sjvayoVlNTo3vvvVelpaXq1atXpJfT7UyZMsX+9+jRo5WVlaWhQ4fq5ZdfVu/evSO4ss7Dr3gkXXbZZerRo8cFr3Cuq6uT2+2O0Kq6n/N7yT5fusLCQm3btk1vvvmmhgwZYp93u91qaWlRQ0NDyHz29uuJj4/XFVdcoczMTJWUlGjMmDF6+umn2dcOqKysVH19va6//nrFxcUpLi5O5eXlWrlypeLi4uRyudjbTpSYmKgrr7xSx48f7zbftwSKPvufU2ZmpsrKyuxz7e3tKisrk8fjieDKupf09HS53e6QfQ4Gg6qoqGCfv4JlWSosLNSWLVu0a9cupaenh4xnZmaqZ8+eIXtbVVWl6upq9vYStLe3q7m5mX3tgEmTJunQoUM6ePCgfYwdO1b5+fn2v9nbznP27Fl9+OGHGjx4cPf5vo30q3RNsWnTJishIcFav369deTIEWv27NlWYmKi5ff7I720qHLmzBnr3Xfftd59911LkvXkk09a7777rvXPf/7TsizLWrZsmZWYmGi9+uqr1nvvvWdNnTrVSk9Ptz799NMIr9xsc+bMsZxOp/XWW29ZJ0+etI///ve/9pxf/vKXVlpamrVr1y5r//79lsfjsTweTwRXHR0WLlxolZeXWydOnLDee+89a+HChVZMTIz1+uuvW5bFvnam/30Xj2Wxtx1x3333WW+99ZZ14sQJ6+2337ays7Otyy67zKqvr7csq3vsLYHyP5555hkrLS3Nio+Pt8aNG2ft3bs30kuKOm+++aYl6YKjoKDAsqzP3mr80EMPWS6Xy0pISLAmTZpkVVVVRXbRUeBieyrJWrdunT3n008/tX71q19ZAwYMsPr06WP9+Mc/tk6ePBm5RUeJX/ziF9bQoUOt+Ph4a9CgQdakSZPsOLEs9rUzfT5Q2NtLd8cdd1iDBw+24uPjre985zvWHXfcYR0/ftwe7w57G2NZlhWZ524AAAAujtegAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjPP/AFgeFJaTFnNyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HISTOGRAM \"THE LENGTH OF A WORD\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCATTER PLOT \"REDUCTION SIZE OF LENGTH\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HISTOGRAM \"COMMUTATOR DEPTH\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HISTOGRAM \"MAX COMMUTEE LENGTH\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE RATE OF WORDS CONTAINING MULTIPLICATION: 0.482\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from itertools import islice\n",
    "\n",
    "freegroups = list(map(lambda x: from_tokenizer(x[0]), visualize))\n",
    "commutators = list(map(lambda x: from_tokenizer(x[1]), visualize))\n",
    "\n",
    "print('HISTORGRAM \"HOW MANY GENERATORS REDUCED\"')\n",
    "\n",
    "diffs = list(map(lambda x: len(to_freegroup(x[1])) - len(x[0]), zip(freegroups, commutators)))\n",
    "plt.hist(diffs)\n",
    "plt.show()\n",
    "\n",
    "print('HISTOGRAM \"THE LENGTH OF A WORD\"')\n",
    "\n",
    "lens = list(map(len, freegroups))\n",
    "plt.hist(lens)\n",
    "plt.show()\n",
    "\n",
    "print('SCATTER PLOT \"REDUCTION SIZE OF LENGTH\"')\n",
    "\n",
    "plt.scatter(lens, diffs)\n",
    "plt.show()\n",
    "\n",
    "print('HISTOGRAM \"COMMUTATOR DEPTH\"')\n",
    "\n",
    "from numpy import arange\n",
    "\n",
    "depths = list(map(calculate_depth, commutators))\n",
    "plt.hist(depths, arange(1, 5 + 1.5) - 0.5)\n",
    "plt.show()\n",
    "\n",
    "print('HISTOGRAM \"MAX COMMUTEE LENGTH\"')\n",
    "\n",
    "lengths = list(map(MaxCommuteeLength(), commutators))\n",
    "plt.hist(lengths, arange(1, 20 + 1.5) - 0.5)\n",
    "plt.show()\n",
    "\n",
    "print(f'THE RATE OF WORDS CONTAINING MULTIPLICATION: {sum(map(FindMultiplication(), commutators)) / len(commutators)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [08:31<00:00, 391.08it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = list(smp.take_unique(int(2 * 10 ** 5), generator, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freegroup_dimension=4;max_commutee_length=3;max_commutator_depth=6;proba_commutator=0.85;proba_multiplication=0.5;max_multipliers_number=3;min_total_length=0;max_total_length=120\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "\n",
    "prefix = config.__str__().replace('DatasetConfig', '').replace('(', '').replace(')', '').replace(', ', ';')\n",
    "\n",
    "with open(path / f'{prefix}.pkl', 'wb') as f:\n",
    "    dump(dataset, f)\n",
    "\n",
    "print(prefix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file = str(path / 'tokenizer.json'),\n",
    "    eos_token = eos_token,\n",
    "    pad_token = pad_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_path = '/home/kibrq/sampling-from-normal-closure/results/commutator-translation/4-free-group/freegroup_dimension=4;max_commutee_length=3;max_commutator_depth=6;proba_commutator=0.85;proba_multiplication=0.5;max_multipliers_number=3;min_total_length=0;max_total_length=120.pkl'\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    dataset = load(f)\n",
    "\n",
    "train_dataset, eval_dataset = train_test_split(dataset, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input):\n",
    "    freegroup_seq, commutator_seq = input\n",
    "    model_inputs = tokenizer(\n",
    "        freegroup_seq,\n",
    "        return_token_type_ids   = False,\n",
    "        return_attention_mask   = False,\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        model_targets = tokenizer(\n",
    "            commutator_seq,\n",
    "        )\n",
    "    \n",
    "    model_inputs['input_ids']   = model_inputs['input_ids']\n",
    "    model_inputs['labels']      = model_targets['input_ids']\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "train_dataset = list(map(preprocess, train_dataset))\n",
    "eval_dataset = list(map(preprocess, eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, GPT2Config, EncoderDecoderConfig, EncoderDecoderModel\n",
    "\n",
    "encoder_config = BertConfig(\n",
    "    vocab_size              = len(tokenizer),\n",
    "    hidden_size             = 128,\n",
    "    max_position_embeddings = 1024,\n",
    "    num_hidden_layers       = 12,\n",
    "    num_attention_heads     = 8,\n",
    "    intermediate_size       = 4 * 128,\n",
    "    pad_token_id            = tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "decoder_config = BertConfig(\n",
    "    vocab_size              = len(tokenizer),\n",
    "    hidden_size             = 128,\n",
    "    max_position_embeddings = 1024,\n",
    "    num_hidden_layers       = 12,\n",
    "    num_attention_heads     = 8,\n",
    "    intermediate_size       = 4 * 128,\n",
    "    pad_token_id            = tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(\n",
    "    encoder_config = encoder_config,\n",
    "    decoder_config = decoder_config,\n",
    ")\n",
    "\n",
    "model = EncoderDecoderModel(config=config)\n",
    "model.config.decoder_start_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id           = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load, combine\n",
    "\n",
    "metrics = combine([load(\"bleu\"), load(\"rouge\")])\n",
    "\n",
    "\n",
    "from re import finditer\n",
    "\n",
    "def trim_eos_tokens(string):\n",
    "    positions = finditer(eos_token, string)\n",
    "    begin = next(positions).end()\n",
    "    \n",
    "    try:                    end = next(positions).start()\n",
    "    except StopIteration:   end = len(string)\n",
    "\n",
    "    return string[begin:end]\n",
    "\n",
    "\n",
    "from numpy import argmax\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    labels[labels == -100] = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "\n",
    "    predictions = tokenizer.batch_decode(logits, skip_special_tokens=False)\n",
    "    references = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "    references = list(map(lambda x: eos_token + ' ' +  x, references))\n",
    "    \n",
    "    predictions = list(map(trim_eos_tokens, predictions))\n",
    "    references = list(map(trim_eos_tokens, references))\n",
    "\n",
    "    print(predictions[0])\n",
    "    print(references[0])\n",
    "\n",
    "\n",
    "    return metrics.compute(references = references, predictions = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "def data_collator(batch: List):\n",
    "    def max_length_pad(batch: List):\n",
    "        max_length = max(map(len, batch))\n",
    "        batch = map(lambda x: x + [tokenizer.pad_token_id] * (max_length - len(x)), batch)\n",
    "        batch = map(lambda x: torch.tensor(x, dtype=int), batch)\n",
    "        batch = torch.stack(list(batch))\n",
    "        \n",
    "        attention_mask = torch.ones_like(batch)\n",
    "        attention_mask.masked_fill_(batch == tokenizer.pad_token_id, 0.) \n",
    "        return batch, attention_mask\n",
    "    \n",
    "    input_ids, attention_mask = max_length_pad([x.input_ids for x in batch])\n",
    "    labels, decoder_attention_mask = max_length_pad([x.labels for x in batch])\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'decoder_attention_mask': decoder_attention_mask,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from /home/kibrq/sampling-from-normal-closure/results/commutator-translation/4-free-group/bert-bert-long/checkpoint-60000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 198000\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 309400\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 38\n",
      "  Continuing training from global step 60000\n",
      "  Will skip the first 38 epochs then the first 1214 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02043890953063965,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1214,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158ec7d093ca437eb706ebdfcfa6047f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkibrq\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kibrq/sampling-from-normal-closure/wandb/run-20230202_112144-12zhwkdg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kibrq/huggingface/runs/12zhwkdg\" target=\"_blank\">results/commutator-translation/4-free-group/bert-bert-long</a></strong> to <a href=\"https://wandb.ai/kibrq/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80762' max='309400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 80762/309400 2:14:11 < 24:37:52, 2.58 it/s, Epoch 52.20/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Precisions</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.094176</td>\n",
       "      <td>0.889908</td>\n",
       "      <td>[0.9864325765235775, 0.925794446034927, 0.8825478718126798, 0.8489306746564728]</td>\n",
       "      <td>0.978468</td>\n",
       "      <td>0.978697</td>\n",
       "      <td>54395</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.977432</td>\n",
       "      <td>0.936652</td>\n",
       "      <td>0.956911</td>\n",
       "      <td>0.956973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>0.093636</td>\n",
       "      <td>0.887356</td>\n",
       "      <td>[0.982575591185299, 0.9201687058287104, 0.874733391263133, 0.8399745044822765]</td>\n",
       "      <td>0.982888</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>54636</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.977974</td>\n",
       "      <td>0.935017</td>\n",
       "      <td>0.956214</td>\n",
       "      <td>0.956210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>0.092590</td>\n",
       "      <td>0.889993</td>\n",
       "      <td>[0.9800007285709081, 0.9191334921175003, 0.8732073395937291, 0.8380025356836122]</td>\n",
       "      <td>0.987745</td>\n",
       "      <td>0.987819</td>\n",
       "      <td>54902</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.977542</td>\n",
       "      <td>0.934064</td>\n",
       "      <td>0.955335</td>\n",
       "      <td>0.955344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.094460</td>\n",
       "      <td>0.888696</td>\n",
       "      <td>[0.9856239429369806, 0.9259867165432476, 0.881181046114771, 0.8460823208529631]</td>\n",
       "      <td>0.978487</td>\n",
       "      <td>0.978715</td>\n",
       "      <td>54396</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.977696</td>\n",
       "      <td>0.935425</td>\n",
       "      <td>0.957515</td>\n",
       "      <td>0.957497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.093579</td>\n",
       "      <td>0.889971</td>\n",
       "      <td>[0.9808053373193095, 0.9192758092283244, 0.8741029119723156, 0.8389037843590741]</td>\n",
       "      <td>0.986961</td>\n",
       "      <td>0.987045</td>\n",
       "      <td>54859</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978089</td>\n",
       "      <td>0.936080</td>\n",
       "      <td>0.957127</td>\n",
       "      <td>0.957182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.093006</td>\n",
       "      <td>0.892108</td>\n",
       "      <td>[0.9817620855849277, 0.9219531605275125, 0.8785843705687727, 0.8445752009184845]</td>\n",
       "      <td>0.985447</td>\n",
       "      <td>0.985552</td>\n",
       "      <td>54776</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.977958</td>\n",
       "      <td>0.936852</td>\n",
       "      <td>0.957566</td>\n",
       "      <td>0.957551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.091450</td>\n",
       "      <td>0.893761</td>\n",
       "      <td>[0.9815074863676982, 0.9224726969886246, 0.8795860956465288, 0.8460467306944075]</td>\n",
       "      <td>0.986487</td>\n",
       "      <td>0.986578</td>\n",
       "      <td>54833</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978169</td>\n",
       "      <td>0.938747</td>\n",
       "      <td>0.958323</td>\n",
       "      <td>0.958292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.092242</td>\n",
       "      <td>0.891501</td>\n",
       "      <td>[0.9848251594457884, 0.925253024883951, 0.8811802863697492, 0.8474590231447162]</td>\n",
       "      <td>0.981570</td>\n",
       "      <td>0.981738</td>\n",
       "      <td>54564</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978724</td>\n",
       "      <td>0.936863</td>\n",
       "      <td>0.957753</td>\n",
       "      <td>0.957768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.091637</td>\n",
       "      <td>0.892537</td>\n",
       "      <td>[0.9822603876096191, 0.921114874453632, 0.8773427206041416, 0.8431697680607587]</td>\n",
       "      <td>0.986779</td>\n",
       "      <td>0.986866</td>\n",
       "      <td>54849</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978735</td>\n",
       "      <td>0.936335</td>\n",
       "      <td>0.958048</td>\n",
       "      <td>0.958131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.092690</td>\n",
       "      <td>0.890817</td>\n",
       "      <td>[0.9797634212920837, 0.9179981114258735, 0.8733267909715408, 0.83926455566905]</td>\n",
       "      <td>0.988618</td>\n",
       "      <td>0.988683</td>\n",
       "      <td>54950</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.977643</td>\n",
       "      <td>0.934812</td>\n",
       "      <td>0.956060</td>\n",
       "      <td>0.956099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.090960</td>\n",
       "      <td>0.894117</td>\n",
       "      <td>[0.9813513021307594, 0.9208278208278208, 0.8785307405224907, 0.8452463708852995]</td>\n",
       "      <td>0.987890</td>\n",
       "      <td>0.987963</td>\n",
       "      <td>54910</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979147</td>\n",
       "      <td>0.936834</td>\n",
       "      <td>0.957699</td>\n",
       "      <td>0.957716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.090107</td>\n",
       "      <td>0.894762</td>\n",
       "      <td>[0.9842065772377065, 0.9259540679672511, 0.8842485634737278, 0.8517977920769689]</td>\n",
       "      <td>0.983017</td>\n",
       "      <td>0.983159</td>\n",
       "      <td>54643</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978888</td>\n",
       "      <td>0.939419</td>\n",
       "      <td>0.959099</td>\n",
       "      <td>0.959207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.090778</td>\n",
       "      <td>0.892758</td>\n",
       "      <td>[0.9818138660142056, 0.921807190099873, 0.8795280398684184, 0.8467816351221112]</td>\n",
       "      <td>0.985283</td>\n",
       "      <td>0.985390</td>\n",
       "      <td>54767</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.977893</td>\n",
       "      <td>0.937119</td>\n",
       "      <td>0.957691</td>\n",
       "      <td>0.957712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.090847</td>\n",
       "      <td>0.892314</td>\n",
       "      <td>[0.9817643619353895, 0.922602791685632, 0.8799282447563476, 0.8464743063536365]</td>\n",
       "      <td>0.984571</td>\n",
       "      <td>0.984688</td>\n",
       "      <td>54728</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.977130</td>\n",
       "      <td>0.935784</td>\n",
       "      <td>0.956490</td>\n",
       "      <td>0.956454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.090553</td>\n",
       "      <td>0.894129</td>\n",
       "      <td>[0.9827000365363536, 0.924440652256352, 0.8816909735908554, 0.848420188756668]</td>\n",
       "      <td>0.984790</td>\n",
       "      <td>0.984904</td>\n",
       "      <td>54740</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978339</td>\n",
       "      <td>0.938882</td>\n",
       "      <td>0.958342</td>\n",
       "      <td>0.958444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.089467</td>\n",
       "      <td>0.896843</td>\n",
       "      <td>[0.9826676163863167, 0.9265634181942259, 0.8852678659389592, 0.852542407646709]</td>\n",
       "      <td>0.985027</td>\n",
       "      <td>0.985138</td>\n",
       "      <td>54753</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978220</td>\n",
       "      <td>0.937861</td>\n",
       "      <td>0.958848</td>\n",
       "      <td>0.958935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.089178</td>\n",
       "      <td>0.894422</td>\n",
       "      <td>[0.9796308779856797, 0.922722029988466, 0.8796942244581131, 0.8464622496778285]</td>\n",
       "      <td>0.987471</td>\n",
       "      <td>0.987549</td>\n",
       "      <td>54887</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978236</td>\n",
       "      <td>0.937959</td>\n",
       "      <td>0.958578</td>\n",
       "      <td>0.958643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.089124</td>\n",
       "      <td>0.897104</td>\n",
       "      <td>[0.9831296735363851, 0.9249479462426652, 0.8836710603974031, 0.8513004300634855]</td>\n",
       "      <td>0.986432</td>\n",
       "      <td>0.986524</td>\n",
       "      <td>54830</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978872</td>\n",
       "      <td>0.938116</td>\n",
       "      <td>0.958816</td>\n",
       "      <td>0.958845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.088070</td>\n",
       "      <td>0.895947</td>\n",
       "      <td>[0.9827126688572472, 0.9247631678666162, 0.883517132729421, 0.8507380073800738]</td>\n",
       "      <td>0.985520</td>\n",
       "      <td>0.985624</td>\n",
       "      <td>54780</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978503</td>\n",
       "      <td>0.938530</td>\n",
       "      <td>0.958942</td>\n",
       "      <td>0.959066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.897917</td>\n",
       "      <td>[0.9820878660325443, 0.9249229372718849, 0.8840975648106292, 0.8518177540457047]</td>\n",
       "      <td>0.987326</td>\n",
       "      <td>0.987405</td>\n",
       "      <td>54879</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978654</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>0.958599</td>\n",
       "      <td>0.958752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.089801</td>\n",
       "      <td>0.895341</td>\n",
       "      <td>[0.9817144315612922, 0.923352001816393, 0.8808896405254464, 0.8486039466142635]</td>\n",
       "      <td>0.986834</td>\n",
       "      <td>0.986920</td>\n",
       "      <td>54852</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978252</td>\n",
       "      <td>0.937445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.958072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.896282</td>\n",
       "      <td>[0.983435717419921, 0.9263131926521937, 0.8853952974593656, 0.8534581895843601]</td>\n",
       "      <td>0.983986</td>\n",
       "      <td>0.984113</td>\n",
       "      <td>54696</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978879</td>\n",
       "      <td>0.939370</td>\n",
       "      <td>0.958588</td>\n",
       "      <td>0.958677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.087037</td>\n",
       "      <td>0.899189</td>\n",
       "      <td>[0.9840666569215027, 0.9281406463359126, 0.8881682699889607, 0.8575972746675423]</td>\n",
       "      <td>0.984571</td>\n",
       "      <td>0.984688</td>\n",
       "      <td>54728</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979597</td>\n",
       "      <td>0.939668</td>\n",
       "      <td>0.959384</td>\n",
       "      <td>0.959420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.088045</td>\n",
       "      <td>0.897846</td>\n",
       "      <td>[0.982590427475338, 0.9271141448615852, 0.8868348443042964, 0.8552318424292162]</td>\n",
       "      <td>0.984790</td>\n",
       "      <td>0.984904</td>\n",
       "      <td>54740</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978892</td>\n",
       "      <td>0.938896</td>\n",
       "      <td>0.959638</td>\n",
       "      <td>0.959711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.087619</td>\n",
       "      <td>0.898866</td>\n",
       "      <td>[0.9858618416233244, 0.9301383925176793, 0.8901865465180618, 0.8589210764546128]</td>\n",
       "      <td>0.982303</td>\n",
       "      <td>0.982457</td>\n",
       "      <td>54604</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979107</td>\n",
       "      <td>0.940055</td>\n",
       "      <td>0.960418</td>\n",
       "      <td>0.960421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.087343</td>\n",
       "      <td>0.898263</td>\n",
       "      <td>[0.9833914328995638, 0.9270898448599193, 0.8857868520013388, 0.8539279785206288]</td>\n",
       "      <td>0.985721</td>\n",
       "      <td>0.985822</td>\n",
       "      <td>54791</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979610</td>\n",
       "      <td>0.940159</td>\n",
       "      <td>0.959996</td>\n",
       "      <td>0.960013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>0.897643</td>\n",
       "      <td>[0.9853264051522248, 0.9285741415982984, 0.8877132027795326, 0.8552285432423545]</td>\n",
       "      <td>0.983254</td>\n",
       "      <td>0.983393</td>\n",
       "      <td>54656</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979728</td>\n",
       "      <td>0.939682</td>\n",
       "      <td>0.960301</td>\n",
       "      <td>0.960310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.084846</td>\n",
       "      <td>0.899458</td>\n",
       "      <td>[0.9837643363284388, 0.9278375919326711, 0.8882890692725983, 0.857268848962179]</td>\n",
       "      <td>0.985082</td>\n",
       "      <td>0.985192</td>\n",
       "      <td>54756</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979318</td>\n",
       "      <td>0.939585</td>\n",
       "      <td>0.959660</td>\n",
       "      <td>0.959602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.084705</td>\n",
       "      <td>0.900437</td>\n",
       "      <td>[0.98206612774015, 0.92661174514398, 0.8863797030869531, 0.8548156324094514]</td>\n",
       "      <td>0.988145</td>\n",
       "      <td>0.988215</td>\n",
       "      <td>54924</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979421</td>\n",
       "      <td>0.941735</td>\n",
       "      <td>0.960412</td>\n",
       "      <td>0.960431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.084396</td>\n",
       "      <td>0.902652</td>\n",
       "      <td>[0.9842037983929875, 0.9307619408642911, 0.892297084318361, 0.8622436423297785]</td>\n",
       "      <td>0.985155</td>\n",
       "      <td>0.985264</td>\n",
       "      <td>54760</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979682</td>\n",
       "      <td>0.942814</td>\n",
       "      <td>0.961444</td>\n",
       "      <td>0.961502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.084852</td>\n",
       "      <td>0.900839</td>\n",
       "      <td>[0.9835599460503773, 0.9276283433586804, 0.8879801832265167, 0.8562190480088405]</td>\n",
       "      <td>0.987089</td>\n",
       "      <td>0.987171</td>\n",
       "      <td>54866</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979996</td>\n",
       "      <td>0.939859</td>\n",
       "      <td>0.960611</td>\n",
       "      <td>0.960682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.084642</td>\n",
       "      <td>0.899567</td>\n",
       "      <td>[0.9826784574710548, 0.9268048065096035, 0.8867538597698889, 0.8554202067765381]</td>\n",
       "      <td>0.986706</td>\n",
       "      <td>0.986794</td>\n",
       "      <td>54845</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979305</td>\n",
       "      <td>0.941642</td>\n",
       "      <td>0.960367</td>\n",
       "      <td>0.960424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.083957</td>\n",
       "      <td>0.898401</td>\n",
       "      <td>[0.9863888175514547, 0.9310279163651598, 0.8913964445500258, 0.8604732654491487]</td>\n",
       "      <td>0.980653</td>\n",
       "      <td>0.980838</td>\n",
       "      <td>54514</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979565</td>\n",
       "      <td>0.941240</td>\n",
       "      <td>0.960802</td>\n",
       "      <td>0.960722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.082844</td>\n",
       "      <td>0.903868</td>\n",
       "      <td>[0.9843992336465651, 0.9315216362086923, 0.8929436079126071, 0.8625140866714476]</td>\n",
       "      <td>0.985976</td>\n",
       "      <td>0.986074</td>\n",
       "      <td>54805</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.980463</td>\n",
       "      <td>0.943867</td>\n",
       "      <td>0.962045</td>\n",
       "      <td>0.962068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.083893</td>\n",
       "      <td>0.901881</td>\n",
       "      <td>[0.9832786287381473, 0.9293906131718395, 0.8897521636506688, 0.8587428337428338]</td>\n",
       "      <td>0.986615</td>\n",
       "      <td>0.986704</td>\n",
       "      <td>54840</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979384</td>\n",
       "      <td>0.941388</td>\n",
       "      <td>0.961179</td>\n",
       "      <td>0.961252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.083047</td>\n",
       "      <td>0.902007</td>\n",
       "      <td>[0.982123006833713, 0.9281702127659575, 0.8896117936117937, 0.8592736572890025]</td>\n",
       "      <td>0.987253</td>\n",
       "      <td>0.987333</td>\n",
       "      <td>54875</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.978919</td>\n",
       "      <td>0.941306</td>\n",
       "      <td>0.960850</td>\n",
       "      <td>0.960816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.081095</td>\n",
       "      <td>0.904282</td>\n",
       "      <td>[0.9820072480923676, 0.9294853622120164, 0.8918897684193986, 0.8623418045020548]</td>\n",
       "      <td>0.987909</td>\n",
       "      <td>0.987981</td>\n",
       "      <td>54911</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979522</td>\n",
       "      <td>0.943204</td>\n",
       "      <td>0.961778</td>\n",
       "      <td>0.961764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.080527</td>\n",
       "      <td>0.904828</td>\n",
       "      <td>[0.9780000724874053, 0.925012222180437, 0.8879293501621663, 0.8588101337887846]</td>\n",
       "      <td>0.992831</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>55182</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979043</td>\n",
       "      <td>0.941473</td>\n",
       "      <td>0.960314</td>\n",
       "      <td>0.960427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>0.902615</td>\n",
       "      <td>[0.9855183763027976, 0.9319415448851774, 0.8933320181495364, 0.8633394947627849]</td>\n",
       "      <td>0.983876</td>\n",
       "      <td>0.984005</td>\n",
       "      <td>54690</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.980331</td>\n",
       "      <td>0.940789</td>\n",
       "      <td>0.960600</td>\n",
       "      <td>0.960555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.081150</td>\n",
       "      <td>0.903069</td>\n",
       "      <td>[0.9829472916286704, 0.9298504637516563, 0.89165846940783, 0.8619291419209503]</td>\n",
       "      <td>0.986432</td>\n",
       "      <td>0.986524</td>\n",
       "      <td>54830</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979205</td>\n",
       "      <td>0.941582</td>\n",
       "      <td>0.960808</td>\n",
       "      <td>0.960867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.081167</td>\n",
       "      <td>0.904286</td>\n",
       "      <td>[0.9850103156780048, 0.9316291144757537, 0.8942506549014201, 0.864386623198212]</td>\n",
       "      <td>0.985356</td>\n",
       "      <td>0.985462</td>\n",
       "      <td>54771</td>\n",
       "      <td>55579</td>\n",
       "      <td>0.979964</td>\n",
       "      <td>0.941807</td>\n",
       "      <td>0.961755</td>\n",
       "      <td>0.961703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9864325765235775, 0.925794446034927, 0.8825478718126798, 0.8489306746564728]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.982575591185299, 0.9201687058287104, 0.874733391263133, 0.8399745044822765]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-61000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-61000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-61000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9800007285709081, 0.9191334921175003, 0.8732073395937291, 0.8380025356836122]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9856239429369806, 0.9259867165432476, 0.881181046114771, 0.8460823208529631]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-62000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-62000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-62000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9808053373193095, 0.9192758092283244, 0.8741029119723156, 0.8389037843590741]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9817620855849277, 0.9219531605275125, 0.8785843705687727, 0.8445752009184845]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-63000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-63000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-63000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9815074863676982, 0.9224726969886246, 0.8795860956465288, 0.8460467306944075]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9848251594457884, 0.925253024883951, 0.8811802863697492, 0.8474590231447162]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-64000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-64000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-64000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9822603876096191, 0.921114874453632, 0.8773427206041416, 0.8431697680607587]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9797634212920837, 0.9179981114258735, 0.8733267909715408, 0.83926455566905]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-65000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-65000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-65000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9813513021307594, 0.9208278208278208, 0.8785307405224907, 0.8452463708852995]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9842065772377065, 0.9259540679672511, 0.8842485634737278, 0.8517977920769689]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-66000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-66000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-66000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9818138660142056, 0.921807190099873, 0.8795280398684184, 0.8467816351221112]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9817643619353895, 0.922602791685632, 0.8799282447563476, 0.8464743063536365]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-67000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-67000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-67000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9827000365363536, 0.924440652256352, 0.8816909735908554, 0.848420188756668]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9826676163863167, 0.9265634181942259, 0.8852678659389592, 0.852542407646709]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-68000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-68000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-68000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9796308779856797, 0.922722029988466, 0.8796942244581131, 0.8464622496778285]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9831296735363851, 0.9249479462426652, 0.8836710603974031, 0.8513004300634855]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-69000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-69000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-69000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9827126688572472, 0.9247631678666162, 0.883517132729421, 0.8507380073800738]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9820878660325443, 0.9249229372718849, 0.8840975648106292, 0.8518177540457047]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-70000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-70000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-70000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9817144315612922, 0.923352001816393, 0.8808896405254464, 0.8486039466142635]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.983435717419921, 0.9263131926521937, 0.8853952974593656, 0.8534581895843601]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-71000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-71000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-71000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9840666569215027, 0.9281406463359126, 0.8881682699889607, 0.8575972746675423]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.982590427475338, 0.9271141448615852, 0.8868348443042964, 0.8552318424292162]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-72000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-72000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-72000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9858618416233244, 0.9301383925176793, 0.8901865465180618, 0.8589210764546128]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9833914328995638, 0.9270898448599193, 0.8857868520013388, 0.8539279785206288]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-73000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-73000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-73000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9853264051522248, 0.9285741415982984, 0.8877132027795326, 0.8552285432423545]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9837643363284388, 0.9278375919326711, 0.8882890692725983, 0.857268848962179]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-74000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-74000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-74000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.98206612774015, 0.92661174514398, 0.8863797030869531, 0.8548156324094514]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9842037983929875, 0.9307619408642911, 0.892297084318361, 0.8622436423297785]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-75000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-75000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-75000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9835599460503773, 0.9276283433586804, 0.8879801832265167, 0.8562190480088405]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9826784574710548, 0.9268048065096035, 0.8867538597698889, 0.8554202067765381]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-76000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-76000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-76000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9863888175514547, 0.9310279163651598, 0.8913964445500258, 0.8604732654491487]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9843992336465651, 0.9315216362086923, 0.8929436079126071, 0.8625140866714476]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-77000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-77000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-77000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9832786287381473, 0.9293906131718395, 0.8897521636506688, 0.8587428337428338]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.982123006833713, 0.9281702127659575, 0.8896117936117937, 0.8592736572890025]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-78000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-78000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-78000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9820072480923676, 0.9294853622120164, 0.8918897684193986, 0.8623418045020548]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9780000724874053, 0.925012222180437, 0.8879293501621663, 0.8588101337887846]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-79000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-79000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-79000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9855183763027976, 0.9319415448851774, 0.8933320181495364, 0.8633394947627849]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9829472916286704, 0.9298504637516563, 0.89165846940783, 0.8619291419209503]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to results/commutator-translation/4-free-group/bert-bert-long/checkpoint-80000\n",
      "Configuration saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-80000/config.json\n",
      "Model weights saved in results/commutator-translation/4-free-group/bert-bert-long/checkpoint-80000/pytorch_model.bin\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n",
      " [ [ 2 3, 3 ], [ 2, 4 -1 ] [ 4, 3 ] [ 1 -3, -4 ] [ 3, -2 -1 -2 ] ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9850103156780048, 0.9316291144757537, 0.8942506549014201, 0.864386623198212]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 35\u001b[0m\n\u001b[1;32m      5\u001b[0m args \u001b[39m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      6\u001b[0m     output_dir                  \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(path\u001b[39m/\u001b[39mprefix),\n\u001b[1;32m      7\u001b[0m     overwrite_output_dir        \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     generation_max_length       \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     27\u001b[0m     model           \u001b[39m=\u001b[39m model,\n\u001b[1;32m     28\u001b[0m     args            \u001b[39m=\u001b[39m args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     compute_metrics \u001b[39m=\u001b[39m compute_metrics,\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m'\u001b[39;49m\u001b[39m/home/kibrq/sampling-from-normal-closure/results/commutator-translation/4-free-group/bert-bert-long/checkpoint-60000\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1495\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1497\u001b[0m )\n\u001b[0;32m-> 1498\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1499\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1500\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1501\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1502\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1503\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1740\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1739\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1743\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1744\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1745\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1746\u001b[0m ):\n\u001b[1;32m   1747\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2488\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2486\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2488\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2490\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DefaultDataCollator\n",
    "\n",
    "prefix = 'bert-bert-long'\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir                  = str(path/prefix),\n",
    "    overwrite_output_dir        = True,\n",
    "\n",
    "    predict_with_generate       = True,\n",
    "    evaluation_strategy         = 'steps',\n",
    "    eval_steps                  = 500,\n",
    "\n",
    "    per_device_train_batch_size = 128,\n",
    "    per_device_eval_batch_size  = 128,\n",
    "    \n",
    "    logging_steps               = 1000,\n",
    "    save_steps                  = 1000,\n",
    "    \n",
    "    num_train_epochs            = 200,\n",
    "\n",
    "    learning_rate               = 8e-5,\n",
    "\n",
    "    generation_max_length       = 128,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model           = model,\n",
    "    args            = args,\n",
    "    train_dataset   = train_dataset,\n",
    "    eval_dataset    = eval_dataset,\n",
    "    data_collator   = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train('/home/kibrq/sampling-from-normal-closure/results/commutator-translation/4-free-group/bert-bert-long/checkpoint-60000')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from transformers import EncoderDecoderModel, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file = '/home/kibrq/sampling-from-normal-closure/results/commutator-translation/4-free-group/tokenizer.json')\n",
    "model = EncoderDecoderModel.from_pretrained('/home/kibrq/sampling-from-normal-closure/results/commutator-translation/4-free-group/bert-bert-long/checkpoint-60000').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "with open('words.pkl', 'rb') as f: words = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from freegroup import tools, sampling as smp\n",
    "from itertools import repeat\n",
    "from re import finditer\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def sampler(word, **generation_kwargs):\n",
    "    def sample():\n",
    "        input_ids = tokenizer(\n",
    "            to_tokenizer(word),\n",
    "        ).input_ids\n",
    "        input_ids = tensor(input_ids, dtype=int, device = model.device)\n",
    "        outputs = model.generate(input_ids.unsqueeze(0), **generation_kwargs)\n",
    "        return tokenizer.batch_decode(outputs)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "from re import finditer\n",
    "\n",
    "def trim_eos_tokens(string):\n",
    "    positions = finditer(eos_token, string)\n",
    "    begin = next(positions).end()\n",
    "    \n",
    "    try:                    end = next(positions).start()\n",
    "    except StopIteration:   end = len(string)\n",
    "\n",
    "    return string[begin:end]\n",
    "\n",
    "\n",
    "def translation(\n",
    "    word,\n",
    "    num_tries = None,\n",
    "    **generation_kwargs,\n",
    "):\n",
    "    g = smp.iterable_from_batches(sampler(\n",
    "        word, **generation_kwargs \n",
    "    ), num_tries)\n",
    "    g = map(trim_eos_tokens, g)\n",
    "    g = map(lambda x: try_or(from_tokenizer, [], x), g)\n",
    "    \n",
    "    return filter(lambda x: normalize(to_freegroup(x)) == word, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[Zyz, p], [Z, YXP]], X]\n",
      "[[[zPZ, Y], [x, yzp]], Z]\n",
      "[1, 2, 3, -2, -1, -4, -3, -4, -3, -2, 3, 4, 4, -3, 2, 3, -4, 3, 4, 1, 2, -3, -2, -1, -3, -2, 3, -4, -4, -3, 2, 3, 4, 1, -4, -3, -2, 3, 4, 4, -3, 2, 3, 1, 2, 3, -2, -1, -4, -3, 4, -3, -2, 3, -4, -4, -3, 2, 3, 4, 3, 4, 1, 2, -3, -2, -1, -4, -1, 4]\n",
      "[[[zPZ, Y], [x, yzp]], XZx]\n",
      "[[Zyz, [P, zpxyZYX]], pYxyP]\n",
      "[[[y, z], [x, yzp]], P]\n",
      "[[[y, z], [P, ZYX]], pXP]\n",
      "[[[xyZYX, Y], [x, yzp]], Xpx]\n",
      "[1, -4, -1, -4, -3, -2, 3, 4, 1, 2, -1, -2, -1, -4, -3, 2, 3, 4, 1, 1, 4, -1, -1, -4, -3, -2, 3, 4, 1, 2, 1, -2, -1, -4, -3, 2, 3, 4, 1, 3, 3, -1, -4, -3, -2, 3, 4, 1, 2, -1, -2, -1, -4, -3, 2, 3, 4, 1, 1, -4, -1, -1, -4, -3, -2, 3, 4, 1, 2, 1, -2, -1, -4, -3, 2, 3, 4, 1, 4, -3, -3, -1]\n",
      "[[[PYp, x], [P, ZYX]], Z[y, z]]\n",
      "[[[xYX, PP], [x, yzp]], z]\n",
      "[-4, -3, -2, -1, 2, 3, 4, 1, 1, 2, -1, -1, -4, -3, -2, 1, 2, 3, 4, 1, -2, -1, 4, 1, 2, -1, -4, -3, -2, -1, 2, 3, 4, 1, 1, -2, -1, -1, -4, -3, -2, 1, 2, 3, -4, -3, 4, -3, -2, -1, 2, 3, 4, 1, 1, 2, -1, -1, -4, -3, -2, 1, 2, 3, 4, 1, -2, -1, -4, 1, 2, -1, -4, -3, -2, -1, 2, 3, 4, 1, 1, -2, -1, -1, -4, -3, -2, 1, 2, 3, 3, 4]\n",
      "[[[P, ZxyZYX], [x, yzp]], y]\n",
      "[[y, [Z, [P, ZYX]]], Yxy]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words:\n\u001b[1;32m     11\u001b[0m     \u001b[39mtry\u001b[39;00m: \n\u001b[0;32m---> 12\u001b[0m         c \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(translation(w, num_tries \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgeneration_config))\n\u001b[1;32m     13\u001b[0m         \u001b[39mprint\u001b[39m(to_lu(c))\n\u001b[1;32m     14\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/freegroup/sampling/utils.py:23\u001b[0m, in \u001b[0;36miterable_from_batches\u001b[0;34m(batch_sampler, num_tries)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39miterable_from_batches\u001b[39m(batch_sampler: Callable[[], List[Word]], num_tries: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable[Word]:\n\u001b[1;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m repeat(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m num_tries \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m repeat(\u001b[39mNone\u001b[39;00m, num_tries):\n\u001b[0;32m---> 23\u001b[0m         \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m batch_sampler():\n\u001b[1;32m     24\u001b[0m             \u001b[39myield\u001b[39;00m word\n",
      "Cell \u001b[0;32mIn [7], line 14\u001b[0m, in \u001b[0;36msampler.<locals>.sample\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     11\u001b[0m     to_tokenizer(word),\n\u001b[1;32m     12\u001b[0m )\u001b[39m.\u001b[39minput_ids\n\u001b[1;32m     13\u001b[0m input_ids \u001b[39m=\u001b[39m tensor(input_ids, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m, device \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgeneration_kwargs)\n\u001b[1;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1360\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1357\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1358\u001b[0m     )\n\u001b[1;32m   1359\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1361\u001b[0m         input_ids,\n\u001b[1;32m   1362\u001b[0m         beam_scorer,\n\u001b[1;32m   1363\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1364\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1365\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1366\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1367\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1368\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1369\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1370\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1374\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1376\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m   1377\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1381\u001b[0m         renormalize_logits\u001b[39m=\u001b[39mrenormalize_logits,\n\u001b[1;32m   1382\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:2263\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2260\u001b[0m next_tokens \u001b[39m=\u001b[39m next_tokens \u001b[39m%\u001b[39m vocab_size\n\u001b[1;32m   2262\u001b[0m \u001b[39m# stateless\u001b[39;00m\n\u001b[0;32m-> 2263\u001b[0m beam_outputs \u001b[39m=\u001b[39m beam_scorer\u001b[39m.\u001b[39;49mprocess(\n\u001b[1;32m   2264\u001b[0m     input_ids,\n\u001b[1;32m   2265\u001b[0m     next_token_scores,\n\u001b[1;32m   2266\u001b[0m     next_tokens,\n\u001b[1;32m   2267\u001b[0m     next_indices,\n\u001b[1;32m   2268\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   2269\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   2270\u001b[0m     beam_indices\u001b[39m=\u001b[39;49mbeam_indices,\n\u001b[1;32m   2271\u001b[0m )\n\u001b[1;32m   2273\u001b[0m beam_scores \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_scores\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2274\u001b[0m beam_next_tokens \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_tokens\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:253\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices)\u001b[0m\n\u001b[1;32m    249\u001b[0m beam_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m beam_token_rank, (next_token, next_score, next_index) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\n\u001b[1;32m    251\u001b[0m     \u001b[39mzip\u001b[39m(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     batch_beam_idx \u001b[39m=\u001b[39m batch_idx \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_size \u001b[39m+\u001b[39;49m next_index\n\u001b[1;32m    254\u001b[0m     \u001b[39m# add to generated hypotheses if end of sentence\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m (eos_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (next_token\u001b[39m.\u001b[39mitem() \u001b[39m==\u001b[39m eos_token_id):\n\u001b[1;32m    256\u001b[0m         \u001b[39m# if beam_token does not belong to top num_beams tokens, it should not be added\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generation_config = dict(\n",
    "    max_length = 40,\n",
    "    num_beams = 100,\n",
    "    num_return_sequences = 50,\n",
    ")\n",
    "\n",
    "\n",
    "failed = []\n",
    "\n",
    "for w in words:\n",
    "    try: \n",
    "        c = next(translation(w, num_tries = 1, **generation_config))\n",
    "        print(to_lu(c))\n",
    "    except StopIteration:\n",
    "        print(w)\n",
    "        failed.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = from_lu('ZYXyzxYXyZYxyzXYxyPZYXyzpxYXyPZYxyzpZYXyzYxyXZYxyzYXyxPZYXyzpYxyXPZYxyzp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PXYxZYXPXpyzpZYzpZYXPxyXpxzXPxyXpxPYZypXPxYXzpZPxYXpxyzPZyzpZYXzPZxyXpxPYzypXPxYXpxZyzPPZYPxpxyzXyxp',\n",
       " 100)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "word = failed[0]\n",
    "to_lu(word), len(word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PXYxZYXPXpyzpZYzpZYXPxyXpxzXPxyXpxPYZypXPxYXzpZPxYXpxyzPZyzpZYXzPZxyXpxPYzypXPxYXpxZyzPPZYPxpxyzXyxp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert normalize(to_freegroup(from_lu('[[[xyZY, [Y, z]] [z, Y], [x, yzp]], Xpx]'))) == normalize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> <s> [',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> [ [',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> [ [ [',\n",
       " '<s> [ [ [ 2, 1 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 2, 1 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ] [ 1, 2 3 ], [ 2, 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 3 4 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ] [ 1, 2 3 4 ], [ 3, 4 ] ] ] <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> [ [ [ [',\n",
       " '<s> [ [ [ 2, 1 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> <s> [',\n",
       " '<s> [ [ [ 2, 1 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> [ [',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, -2 1 2 3 4 ], [ 3, 4 ] ] ] <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ] [ 1, 2 3 4 ], [ 2, 3 4 ] ] ] <s>',\n",
       " '<s> [ [ [ -2 1 2, 3 ], [ 2, 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 2, 1 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> <s> [',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 3 4 ], [ 2 3, 4 ] ] ] <s> <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ] [ 1, 2 3 ], [ 2, 3 ] ], [ [ 1, 2 3 4 ], [ 1, 2 3 4 ] ] ]',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 2, 1 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> [ [',\n",
       " '<s> [ [ [ 2, 1 ] [ 1, 2 3 ], [ 2, 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ] [ 1, 2 3 ], [ 2, 3 ] ], [ [ 1, 2 3 4 ], [ 2 3, 4 ] ] ] <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> [ [ [ [ [',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ [ 1, 2 ], 3 4 ], [ 3, 4 ] ] ] <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ] [ 1, 2 3 4 ], [ 2 3, 4 ] ] ] <s>',\n",
       " '<s> [ [ [ 2, 1 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> [ [ [',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> [ [ 2',\n",
       " '<s> [ [ [ 1, 2 ] [ 1, 2 3 ], [ 2, 3 ] ], [ [ 2, 1 ], [ 1, 2 3 4 ] ] ] <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ -2 1 2, 3 4 ], [ 3, 4 ] ] ] <s> <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 2, 1 ], [ 1, 2 3 ] ], [ [ 2, 1 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ] [ 1, 2 3 ], [ 2, 3 ] ], [ [ 1, 2 3 4 ], [ 2, 3 4 ] ] ] <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> [ [ [ 1',\n",
       " '<s> [ [ [ 1, 2 ] [ 3, 2 ], [ 2, 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 2, 1 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> [ [ [',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> [ [ 3',\n",
       " '<s> [ [ [ [ 1, 2 ], 3 ], [ 2, 3 ] ], [ [ 1, 2 3 4 ], [ 2 3, 4 ] ] ] <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> [ [ 2,',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> [ [ 1',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> [ [ [ 2',\n",
       " '<s> [ [ [ [ 2, 1 ], 3 ], [ 1, 2 3 ] ], [ [ 1, 2 3 4 ], [ 2 3, 4 ] ] ] <s> <s>',\n",
       " '<s> [ [ [ 2, 1 ], [ 1, 2 3 ] ], [ [ 1, 2 3 4 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ [ 2, 1 ], 3 ], [ 1, 2 3 ] ], [ [ 1, 2 3 4 ], [ 2, 3 4 ] ] ] <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 3 4 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> [',\n",
       " '<s> [ [ [ [ 1, 2 ], 3 ], [ 2, 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, -2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 2 1, 3 ], [ 1, 2 3 ] ], [ [ 1, 2 3 4 ], [ 2 3, 4 ] ] ] <s> <s> <s> <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> [ [ 4',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> [ [ [ 1,',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> <s> [ 4',\n",
       " '<s> [ [ [ [ 1, 2 ], 3 ], [ 1, 2 3 ] ], [ [ 1, 2 3 4 ], [ 2 3, 4 ] ] ] <s> <s>',\n",
       " '<s> [ [ [ 1, 2 ], [ 1, 2 3 ] ], [ [ 1, 2 ], [ 1, 2 3 4 ] ] ] <s> <s> <s> <s> [ [ -2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = dict(\n",
    "    max_length = 40,\n",
    "    num_beams = 100,\n",
    "    num_return_sequences = 50,\n",
    ")\n",
    "\n",
    "# generation_config = dict(\n",
    "#     max_length = 40,\n",
    "#     do_sample = True,\n",
    "#     temperature = 1.5,\n",
    "#     num_return_sequences = 100,\n",
    "# )\n",
    "s = sampler(word, **generation_config)\n",
    "s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert normalize(to_freegroup(from_tokenizer('[ -1, [ 2 3 4 -3 -4 -2, [ 2, 3 4 1 ] ] ] '))) == word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from functools import reduce\n",
    "from random import sample\n",
    "\n",
    "Word = List[int]\n",
    "\n",
    "def remove_complementaries(word: Word):\n",
    "    if len(word) >= 2 and word[-1] == -word[-2]:\n",
    "        return word[:-2]\n",
    "    return word\n",
    "\n",
    "def remove_rotations(word: Word, *substrs: Word):\n",
    "    for substr in substrs:\n",
    "        if len(word) < len(substr): continue\n",
    "        doubled = substr * 2\n",
    "        for idx in range(len(doubled)):\n",
    "            if word[-len(substr):] == doubled[idx:idx + len(substr)]:\n",
    "                return word[:-len(substr)]\n",
    "    return word\n",
    "\n",
    "def combine(x, y):\n",
    "    x.append(y)\n",
    "    return x\n",
    "\n",
    "def generate_from_intersection(\n",
    "    prefix: Word,\n",
    "    num_generators: int,\n",
    "    max_iterations: int,\n",
    "):\n",
    "    generated = prefix[::]\n",
    "\n",
    "    stacks = [[] for _ in range(num_generators + 1)]\n",
    "    bases = [[i] for i in range(1, num_generators + 1)] + [list(range(1, num_generators + 1))]\n",
    "    ibases = [[-f for f in base[::-1]] for base in bases]\n",
    "\n",
    "    for i, (base, ibase) in enumerate(zip(bases, ibases)):\n",
    "        stacks[i] = reduce(lambda x, y: remove_complementaries(remove_rotations(combine(x, y), base, ibase)), prefix, [])\n",
    "\n",
    "    iteration = 0\n",
    "    while not all(map(lambda v: len(v) == 0, stacks)) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "\n",
    "        votes = {}\n",
    "        for stack, base, ibase in zip(stacks, bases, ibases):\n",
    "            if len(stack) == 0: continue\n",
    "\n",
    "            last = stack[-1]\n",
    "            if last == generated[-1]:\n",
    "                if not last in base and not last in ibase:\n",
    "                    for key in [base[0], ibase[0]]:\n",
    "                        votes[key] = votes.get(key, 0) + 1\n",
    "                elif last in base:\n",
    "                    idx = base.index(last)\n",
    "                    key = base[(idx + 1) % len(base)]\n",
    "                    votes[key] = votes.get(key, 0) + 1\n",
    "                elif last in ibase:\n",
    "                    idx = ibase.index(last)\n",
    "                    key = ibase[(idx + 1) % len(ibase)]\n",
    "                    votes[key] = votes.get(key, 0) + 1\n",
    "            else:\n",
    "                votes[-last] = votes.get(-last, 0) + 1\n",
    "\n",
    "        max_val  = max(votes.values())\n",
    "        max_key  = sample([key for key in votes.keys() if votes[key] == max_val], k = 1)[0]\n",
    "\n",
    "        generated.append(max_key)\n",
    "        \n",
    "        for i, (base, ibase) in enumerate(zip(bases, ibases)):\n",
    "            stacks[i] = remove_complementaries(remove_rotations(combine(stacks[i], max_key), base, ibase))\n",
    "    \n",
    "    \n",
    "    if not all(map(lambda v: len(v) == 0, stacks)):\n",
    "        return None\n",
    "    \n",
    "    return generated   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:35<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "from tqdm import trange\n",
    "\n",
    "number_generators = 4\n",
    "max_prefix_length = 20\n",
    "\n",
    "tries = 200\n",
    "max_iterations = 150\n",
    "\n",
    "num_samples = 20\n",
    "\n",
    "unique = set()\n",
    "\n",
    "for _ in trange(num_samples):\n",
    "    \n",
    "    generated = None\n",
    "    while generated is None or tuple(generated) in unique:\n",
    "        prefix = None\n",
    "        while prefix is None or len(prefix) == 0:\n",
    "            prefix = randint(-number_generators, number_generators + 1, size = randint(max_prefix_length)).tolist()\n",
    "            prefix = [f for f in prefix if f != 0]\n",
    "            prefix = reduce(lambda x, y: remove_complementaries(combine(x, y)), prefix, [])\n",
    "        \n",
    "        iteration = 0\n",
    "        while generated is None and iteration < tries:\n",
    "            generated = generate_from_intersection(prefix, number_generators, max_iterations)\n",
    "            iteration += 1\n",
    "    unique.add(tuple(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4, -1, -2, 1, -3, -2, -1, -4, -1, 4, 2, 3, 4, -3, -2, 3, 4, -3, -2, -1, -4, 1, 2, -1, 4, 1, 3, -1, -4, 1, 2, -1, 4, 1, -4, -2, -3, 2, 4, -1, -4, 1, -2, -1, 3, 4, -3, -4, 1, -2, -1, 4, 1, 2, 3, -4, -3, 2, 3, 4, -3, -2, -1, 3, -4, -3, 1, 2, -1, 4, 1, -4, -2, 3, 2, 4, -1, -4, 1, -2, -1, 4, 1, -3, 2, 3, -4, -4, -3, -2, -4, 1, 4, 1, 2, 3, -1, 2, 1, 4]\n",
      "[4, 1, 3, -2, 3, 1, 3, 4, -1, -4, -4, -1, 2, -3, -2, -1, 2, 3, -2, 4, -3, -2, -1, -4, 1, 2, 3, 2, -3, -2, 1, 2, 3, -2, -3, -1, -2, -1, 4, 1, 2, 3, -4, -3, 1, 3, 1, 4, 4, 1, -4, -3, -1, -3, 2, -3, -1, -4]\n",
      "[[[Y, Z], x], [pxy, xP][P, Zx][x, y]]\n",
      "[-1, -4, 1, 2, -1, -4, -3, -2, 3, 4, 4, 1, -4, -3, 4, -1, -4, 1, 3, -1, -4, -3, 2, 3, 4, 1, -2, -3, -1, 4, 1, -4, 3, 4]\n",
      "[1, 2, 3, 4, -3, -2, -1, 2, 3, -2, 1, 2, -4, -2, -1, 2, -3, -2, 3, 1, 4, -1, -3, 1, 3, -4, -3, -1, -4, -3, -2, 3, 4, -3, -1, 3, 1, -4, -1, -3, 2, 3, -2, 1, 2, 4, -2, -1, 2, -3, -2, 1, 2, 3]\n",
      "[[X, [pYP, [P, z]]], [X, z][z, yzp]]\n",
      "[1, 2, 3, -2, -1, -4, -3, 4, -2, -1, 2, 3, -2, -1, -4, -3, 4, 1, 1, 2, -1, -4, 1, -2, -1, 2, 4, -2, -1, -4, 3, 4, 1, 2, -3, -4, -2, 1, 2, -1, 4, 1, -4, 3, 4, 1, 2, -3, -2, -1]\n",
      "[3, -4, -2, 4, 1, 2, -1, -4, -3, 4, 1, -4, -3, 4, 1, -4, -3, -2, -1, 2, 3, 3, 4, -3, -2, 3, -4, -3, 4, 2, -4, -3, -2, 1, 2, 3, 4, -1, -2, -4, 3, 4, -3, 2, 3, -1, -4, 3, 4, 1, -2, -1, -4, 2, 4, -3]\n",
      "[Zx, Y][[p, [Z, Y]], [yzp, X]][Y, Zx]\n",
      "[4, -3, 2, 3, -4, -3, -2, 3, 2, 4, -2, -1, 2, 3, 4, 1, -4, -3, -4, -2, -3, 2, 3, 4, -3, -2, 3, 2, 3, 4, -1, -4, -3, -2, 1, -4]\n",
      "[-2, 1, 2, 3, 4, 2, -4, -3, -1, -4, 1, 2, 3, -2, -1, 2, -3, -2, 4, 1, 4, -1, -4, -3, 4, 1, -4, -3, -2, 3, 4, 1, 2, -1, -1, -4, 3, 4, 1, -4, -1, -3, 1, 4, 1, -2, -1, -4, -3, 2, 3, -1, 3, -4, 2, 3, -2, 1, 2, -3, -2, -1, 4, 1, 3, 4, -2, -4, -3, -2, -1, 2]\n",
      "[4, 3, -4, 1, 1, 1, 2, -1, -4, -3, -2, 3, 4, -1, -1, 4, -3, -4, 1, 4, 3, -4, -3, -1, 3, 1, 1, -4, -3, 2, 3, 4, 1, -2, -1, -1, -1, -3, 1, 3, 4, -3, -4, -1]\n",
      "[2, 4, -2, -1, -4, -3, 4, 1, 2, -4, -2, -1, 3, 1, 2, -1, -4, -3, -2, 3, 4, -3, 1, 4, -1, -4, 3, 4, 1, -4, -1, -4, -3, 2, 3, 4, 1, -2, 4, -1, -4, -3, 4, 1, -4, 2, 4, -2, -1, -4, 3, 4, 1, 2, -4, -2]\n",
      "[-4, 2, 2, 3, -2, -1, 2, 3, 4, 1, -4, -3, -3, -2, -2, 4, 2, 3, -2, -4, 2, 4, -3, -4, 2, 3, 3, 4, -1, -4, -3, -2, 1, 2, -3, -2, 4, 3, -4, -2, 4, 2, -3, -2]\n",
      "[-2, -4, 3, -1, 3, 2, -3, -4, -3, -2, 3, 4, 1, -3, -1, -4, 1, 2, 3, 4, -3, -2, 3, -1, -3, 2, 3, -2, 1, 2, -4, -3, -2, -1, 4, 1, 2, 3, -2, -1, 2, -3, -2, 3, -4, -3, 2, 3, 4, 3, -2, -3, 1, -3, 4, 2]\n",
      "[1, 2, -3, -3, -4, 3, 1, 2, 2, 3, 4, -3, -2, 3, 4, -3, -2, -1, 2, 3, -4, -3, -2, 3, 4, -3, -4, 2, 4, 1, -4, -3, -2, -1, 2, 3, -2, 4, 3, -4, -3, 2, 3, 4, -3, -4, -3, -2, 1, 2, 3, 4, -1, -2, 1, 2, 3, -4, -3, 2, 3, -4, -3, -2, -2, -1, -3, 4, 3, 3, -2, -1]\n",
      "[-4, 2, 3, 2, -4, 2, 2, 4, -2, -4, -1, 4, 1, -2, -1, -4, 1, 4, -3, 4, 1, 2, 3, -2, -1, -4, -4, -1, 4, 1, 2, -1, -4, 1, 4, -2, 4, 1, 2, -3, -2, -1, -4, 3, 2, 2, -4, -2, -2, 4, -2, -3, -2, 4]\n",
      "[1, -2, -1, -1, -2, -2, -2, -1, 2, 3, -2, -1, -4, -3, 4, 1, -4, -3, 2, 3, 4, -3, -2, 3, -1, -4, 3, 4, 1, 2, -3, -2, 1, 2, 3, -2, -1, -4, -3, 4, -3, 2, 3, -4, -3, -2, 3, 3, 4, 1, 2, -3, 2, 2, 1, 1, 2, -1]\n",
      "[[y, z][pzP, y], [pXPZY, Z][Z, YX]]\n",
      "[3, 4, -3, -2, -1, -4, 1, 2, -1, 4, 1, 3, -1, -4, 1, 2, -1, -4, -3, -2, 3, 4, 4, 1, -4, -3, 4, -1, -4, 1, 3, -1, -4, -3, 2, 3, 4, 1, -2, -3, -1, 4, 1, -4, 3, 4, -3, -1, -4, 1, -2, -1, 4, 1, 2, 3, -4, -3]\n"
     ]
    }
   ],
   "source": [
    "generation_config = dict(\n",
    "    max_length = 40,\n",
    "    num_beams = 100,\n",
    "    num_return_sequences = 50,\n",
    ")\n",
    "\n",
    "\n",
    "failed = []\n",
    "\n",
    "for w in unique:\n",
    "    w = list(w)\n",
    "    try: \n",
    "        c = next(translation(w, num_tries = 1, **generation_config))\n",
    "        print(to_lu(c))\n",
    "    except StopIteration:\n",
    "        print(w)\n",
    "        failed.append(w)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
