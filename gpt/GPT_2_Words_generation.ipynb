{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCLfkWyFEVLQ",
        "outputId": "260e7e94-b87f-4b73-ecf6-1fe1f7ac65fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "1G07UhFcGS6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)\n",
        "import itertools\n",
        "\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dayIw6hdEbpO",
        "outputId": "cb328b7b-5f05-4bb8-fc3a-8da40b5d434d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "TP869jFFKMSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transforms as tf\n",
        "import generators as gens\n",
        "from generators import from_free_group, from_normal_closure, uniform_hyperbolic_length\n",
        "from free_group import is_from_normal_closure\n",
        "\n",
        "def check_in_closer(generated_words):\n",
        "  alg_words = []\n",
        "  for generated_word in generated_words:\n",
        "    output = tokenizer.decode(generated_word, skip_special_tokens=True)\n",
        "    output1 = (list(map(int, [i for i in [i for i in output]])))\n",
        "    output1 = list(map(int, [words[x] if words else x for x in output1]))\n",
        "    if (is_from_normal_closure([1], output1) == True):\n",
        "      alg_words.append(output1)\n",
        "      #print(word_as_str(output1))\n",
        "  print(len(alg_words)*100/len(generated_words), '%')\n",
        "\n",
        "def word_as_str(word):\n",
        "    letters = \"xyzpqrstuvwklme\"\n",
        "    return \"\".join(map(lambda factor: letters[abs(factor) - 1] + (\"⁻¹\" if factor < 0 else \"\"), word))\n",
        "\n",
        "def print_word(word_):\n",
        "  a = (list(map(int, [i for i in [i for i in tokenizer.decode(word_, skip_special_tokens=True)]])))\n",
        "  print(word_as_str(list(map(int, [words[x] if words else x for x in a]))))"
      ],
      "metadata": {
        "id": "1fAB8Z9TEbr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium"
      ],
      "metadata": {
        "id": "3uuldLLpEbu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12cNyk8CEbxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "f = open('/content/drive/MyDrive/Folder/datasets/_xyzp_-10.json')\n",
        "data = json.load(f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "C9gImbepEbz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_max_list(list):\n",
        "    list_len = [len(i) for i in list]\n",
        "    print(max(list_len))\n",
        "\n",
        "find_max_list(data)"
      ],
      "metadata": {
        "id": "6tb3DOZSGw7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentenses = []\n",
        "for element in data_train:\n",
        "  results = list(map(str, element['data']))\n",
        "  if element['label'][0] == 1:\n",
        "    sentenses.append(results)"
      ],
      "metadata": {
        "id": "3vFncLxHGtM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentenses = data\n",
        "merged = list(itertools.chain(*sentenses))\n",
        "words = list(set(merged))  #коллекция уникальных элементов\n",
        "word2idx = dict((word, i) for i, word in enumerate(words))\n",
        "token_stream_data = []\n",
        "for i in sentenses:\n",
        "  C = (pd.Series(i)).map(word2idx) \n",
        "  token_stream_data.append(list(C))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dD5gySHE1dh",
        "outputId": "949176fb-3280-4dfb-a0f9-c7668c00bc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXyxCCt5E1gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CpX3JpAqE1iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(token_data):\n",
        "  token_seq = []\n",
        "  for xs in token_data:     \n",
        "    a = \"\".join(str(x) for x in xs)\n",
        "    token_seq.append(a)\n",
        "  return token_seq"
      ],
      "metadata": {
        "id": "_eD3gGyIGkPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = tokenization(token_stream_data)"
      ],
      "metadata": {
        "id": "uJQjLrmzGkUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "    for txt in txt_list:\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx] "
      ],
      "metadata": {
        "id": "v1gObM25GkZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 14\n",
        "max_length = 160"
      ],
      "metadata": {
        "id": "yOsdS45zGkb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = GPT2Dataset(train_texts, tokenizer, max_length=max_length)#768)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "train_dataloader = DataLoader( train_dataset,   sampler = RandomSampler(train_dataset),   batch_size = batch_size)\n",
        "validation_dataloader = DataLoader(  val_dataset,   sampler = SequentialSampler(val_dataset),  batch_size = batch_size)"
      ],
      "metadata": {
        "id": "Sa0TcyaJGf4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "model1 = GPT2LMHeadModel.from_pretrained('gpt2', config=configuration)\n",
        "model1.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "4EzS9u3CGf7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model1\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "4CSe-12GGf9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 6\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "sample_every = 200\n",
        "optimizer = AdamW(model.parameters(),  lr = learning_rate,   eps = epsilon )\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,   num_warmup_steps = warmup_steps, num_training_steps = total_steps)\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "metadata": {
        "id": "2XqtJDq_GgAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_t0 = time.time()\n",
        "training_stats = []\n",
        "model = model.to(device)\n",
        "for epoch_i in range(0, epochs):\n",
        "    #               Training\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        #tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "\n",
        "        #b_labels[b_labels == 2598] = -100\n",
        "        #bool_mask_invert = ~bool_mask.bool()\n",
        "        #b_labels[bool_mask_invert] = -100\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        \n",
        "        outputs = model(  b_input_ids,\n",
        "                          labels=b_labels, \n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None )\n",
        "        loss = outputs[0]  \n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "        # Get sample every x batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "\n",
        "            model.eval()\n",
        "            prompt = \"<|startoftext|>\"\n",
        "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "            generated = generated.to(device)\n",
        "            sample_outputs = model.generate(generated,\n",
        "                                    #bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=200,\n",
        "                                     #pad_token_id = 50258\n",
        "                                      )\n",
        "\n",
        "            \n",
        "            #performance_estimate_inters(sample_outputs)\n",
        "            \n",
        "            a = []\n",
        "            for generated_word in sample_outputs:\n",
        "                output1 = [1,2,3]\n",
        "                try:\n",
        "                  output = tokenizer.decode(generated_word, skip_special_tokens=True)\n",
        "                  output1 = (list(map(int, [i for i in [i for i in output]])))\n",
        "                  output1 = list(map(int, [words[x] if words else x for x in output1]))\n",
        "                except: \n",
        "                  print('error')\n",
        "                  pass\n",
        "                a.append(output1)\n",
        "            eavluate_sampled(a, 5)\n",
        "            for i, sample_output in enumerate(sample_outputs[:3]):\n",
        "                #print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "                #print(sample_output)\n",
        "                pass\n",
        "                #output = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "                #print(output)\n",
        "                #output1 = (list(map(int, [i for i in [i for i in output]])))\n",
        "                #output1 = list(map(int, [words[x] if words else x for x in output1]))\n",
        "                #print(word_as_str(output1))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))       \n",
        "    #               Validation\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        #b_labels[b_labels == 2816] = -100\n",
        "        b_masks = batch[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs  = model(b_input_ids, \n",
        "                          #  token_type_ids=None, \n",
        "                             attention_mask = b_masks,\n",
        "                            labels=b_labels)\n",
        "            loss = outputs[0]  \n",
        "            \n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss        \n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)    \n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "          { 'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time})\n",
        "    \n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "id": "QBPPfYNyEb2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = GPT2Config.from_pretrained('/content/drive/MyDrive/Folder/model_gpt2_<xyzp>_130/config.json', output_hidden_states=False)\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/Folder/model_gpt2_<xyzp>_130/pytorch_model.bin', config=configuration)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/drive/MyDrive/Folder/model_gpt2_<xyzp>_130', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') "
      ],
      "metadata": {
        "id": "7Uk_pORzEb5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "start_time = datetime.now()\n",
        "device = torch.device(\"cuda\")\n",
        "#model.cuda()\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "prompt = \"<|startoftext|>\"\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "examples_number = 600\n",
        "sample_outputs = model.generate(generated, do_sample=True, top_k=10, max_length = 100, \n",
        "                                top_p=0.95, num_return_sequences= examples_number)\n",
        "print(datetime.now() - start_time)"
      ],
      "metadata": {
        "id": "wWANIj3-Eb7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mSnWsP3PEb-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "for generated_word in sample_outputs:\n",
        "    output = tokenizer.decode(generated_word, skip_special_tokens=True)\n",
        "    output1 = (list(map(int, [i for i in [i for i in output]])))\n",
        "    output1 = list(map(int, [words[x] if words else x for x in output1]))\n",
        "    output1 = normalize(output1)\n",
        "    a.append(output1)\n",
        "\n",
        "alg4, alg4_ = eavluate_sampled(a, 4)"
      ],
      "metadata": {
        "id": "txXEGnsBE9iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bF-lzEKeFNgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_as_str(word) -> str:\n",
        "    letters = \"xyzpqrstuvwklmn\"\n",
        "    inverse_letters = letters.upper()\n",
        "    return ''.join([letters[abs(f) - 1] if f >= 0 else inverse_letters[abs(f) - 1] for f in word])\n",
        "\n",
        "def eavluate_sampled(sampled: list, n_generators: int):\n",
        "    bases   = [[i] for i in range(1, n_generators + 1)] + [list(range(1, n_generators + 1))]\n",
        "    uniques = [set() for _ in range(n_generators + 1)]\n",
        "\n",
        "    length_threshold = 20\n",
        "\n",
        "    for seq in sampled:\n",
        "        word = word_as_str(seq)\n",
        "        for base, unique in zip(bases, uniques):\n",
        "            #unique = list(unique)\n",
        "            for t in range(length_threshold, len(word)):\n",
        "                if is_from_normal_closure(base, seq[:t+1]):# and (seq[:t+1] not in data):\n",
        "                    unique.add(word[:(t+1)])\n",
        "\n",
        "    a = []\n",
        "    for i in uniques:\n",
        "      a.append(set.intersection(i))\n",
        "    print('1',len(set.union(*a)))  \n",
        "    print('2',len(set.union(set.intersection(*uniques[0:2]), set.intersection(*uniques[1:3]), set.intersection(uniques[0], uniques[2]))))\n",
        "    print('symmetric commutant',len(set.intersection(*uniques[:3])))\n",
        "    print('full',len(set.intersection(*uniques[:])))\n",
        "    #print(set.intersection(*uniques[:]))\n",
        "    return set.intersection(*uniques[:4]), set.intersection(*uniques[:])\n",
        "\n",
        "def normalize(word):\n",
        "    normalized = []\n",
        "\n",
        "    for factor in word:\n",
        "        if factor == 0:\n",
        "            continue\n",
        "        if len(normalized) == 0:\n",
        "            normalized.append(factor)\n",
        "            continue\n",
        "\n",
        "        if factor == -normalized[-1]:\n",
        "            normalized.pop()\n",
        "        else:\n",
        "            normalized.append(factor)\n",
        "\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "7ywBewe2E9k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yK-C0aO6E9nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34iCi7sKE9qA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}